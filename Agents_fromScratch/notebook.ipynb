{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8214420c-1936-472c-930e-1f4c9efb3809",
   "metadata": {},
   "source": [
    "# GenAI with Python: Agents from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53219017-c57b-4ae8-8442-b89f95ff8bdc",
   "metadata": {},
   "source": [
    "###### [Article: TowardsDataScience]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd5a8e7-d0ca-4c6c-88b4-1c1e27591b84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 0 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "10f53cd6-8254-40ff-aca4-ca18a1bc7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain --> 0.2.14\n",
    "#pip install langgraph --> 0.2.19\n",
    "#pip install ollama --> 0.3.1\n",
    "#pip install semantic-router --> 0.0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac37da15-9f5f-46b7-99cb-7bb4301ef088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.1',\n",
       " 'created_at': '2024-09-11T14:26:15.101194Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"I don't have any information about deaths that occurred on September 9, 2024. My knowledge cutoff is March 1, 2023, and I do not have real-time access to information. If you're looking for information on a specific person or event, I'd be happy to try to help with what I know up to my cutoff date.\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 12084964522,\n",
       " 'load_duration': 31783841,\n",
       " 'prompt_eval_count': 22,\n",
       " 'prompt_eval_duration': 164199000,\n",
       " 'eval_count': 74,\n",
       " 'eval_duration': 11883766000}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "llm = \"llama3.1\"\n",
    "q = '''who died on September 9, 2024?'''\n",
    "\n",
    "res = ollama.chat(model=llm, \n",
    "                  messages=[{\"role\":\"system\", \"content\":\"\"},\n",
    "                            {\"role\":\"user\", \"content\":q}])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c648ef-bed3-4179-8dcb-d6678abc0faa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1 - Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6aa2dbe-0aa0-4d57-964b-b8c7aac331e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94aab20-4864-49cf-a1df-f971e1ede1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search in the United States of people who died on September 9, 2024 obituaries and condolences. Find an obituary, get service details, leave condolence messages or send flowers or gifts in memory of a loved one. FILE - James Earl Jones arrives before the 84th Academy Awards on Sunday, Feb. 26, 2012, in the Hollywood section of Los Angeles. Jones, who overcame racial prejudice and a severe stutter to become a celebrated icon of stage and screen has died at age 93. His agent, Barry McPherson, confirmed Jones died Monday morning, Sept. 9, 2024, at home. James Earl Jones, revered actor who voiced Darth Vader in Star Wars, starred in Field of Dreams' died September 9 at his home in Dutchess County, NY. He was 93. ... September 9, 2024 1:33pm. Events. Deaths. Sep 1 John Schultz, Australian Football HOF ruckman (Brownlow Medal 1960; All Australian 1961; Victoria 24 games; Footscray FC), dies at 85. Sep 2 Aleksandr Medved, Ukrainian freestyle wrestler (Olympic gold USSR heavyweight 1964, 68, 72; World C'ship gold x 7), dies at 86. Sept. 11, 2024, 5:03 a.m. ET. ... \"Since September 11, 2001, ... More firefighters have now died from 9/11-related illnesses than in the attacks that day (343). The Fire Department also says ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "def browser(q: str) -> str:\n",
    "    \"\"\"Search on DuckDuckGo browser\"\"\"\n",
    "    return DuckDuckGoSearchRun().run(q)\n",
    "\n",
    "print( browser(q) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3950dd1b-f78d-4e12-b279-68f113242ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'browser',\n",
       "  'description': 'Search on DuckDuckGo browser',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'q': {'description': None, 'type': 'string'}},\n",
       "   'required': []}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_router.utils.function_call import FunctionSchema\n",
    "\n",
    "tool_browser = FunctionSchema(browser).to_ollama()\n",
    "tool_browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccb04de-9059-4d23-bc5c-8a84c02be92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'yo', 'add_tool': '', 'add_source': ''}\n"
     ]
    }
   ],
   "source": [
    "def final_answer(answer:str, add_tool:str=\"\", add_source:str=\"\"):\n",
    "    \"\"\"Returns a natural language response to the user. There are 3 sections to be returned to the user:\n",
    "    - `answer`: the final natural language answer to the user's question, should provide as much context as possible.\n",
    "    - `add_tool`: additional information regarding which tool you used to get the answer.\n",
    "    - `add_source`: additional information regarding what's the source of the answer.\n",
    "    \"\"\"\n",
    "    return {\"answer\":answer, \"add_tool\":add_tool, \"add_source\":add_source}\n",
    "\n",
    "print( final_answer(\"yo\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da5fd0c-fab4-4920-a078-d701eeac88c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'final_answer',\n",
       "  'description': \"Returns a natural language response to the user. There are 3 sections to be returned to the user:\\n- `answer`: the final natural language answer to the user's question, should provide as much context as possible.\\n- `add_tool`: additional information regarding which tool you used to get the answer.\\n- `add_source`: additional information regarding what's the source of the answer.\",\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'answer': {'description': None, 'type': 'string'},\n",
       "    'add_tool': {'description': None, 'type': 'string'},\n",
       "    'add_source': {'description': None, 'type': 'string'}},\n",
       "   'required': ['add_tool', 'add_source']}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_final_answer = FunctionSchema(final_answer).to_ollama()\n",
    "tool_final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182ae77-bed0-4262-b559-3f9c5a4de070",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2 - Decision Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "952d7dad-b331-4e74-8f2d-6301d0d39f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You know everything, you must answer every question from the user, you can use the list of tools provided to you.\n",
    "Your goal is to provide the user with the best possible answer, including key information about the sources and tools used.\n",
    "\n",
    "Note, when using a tool, you provide the tool name and the arguments to use in JSON format. \n",
    "For each call, you MUST ONLY use one tool AND the response format must ALWAYS be in the pattern:\n",
    "```json\n",
    "{\"name\":\"<tool_name>\", \"parameters\": {\"<tool_input_key>\":<tool_input_value>}}\n",
    "```\n",
    "Remember, do NOT use any tool with the same query more than once.\n",
    "Remember, if the user doesn't ask a specific question, you MUST use the `final_answer` tool directly.\n",
    "\n",
    "Every time the user asks a question, you take note of some keywords in the memory.\n",
    "Every time you find some information related to the user's question, you take note of some keywords in the memory.\n",
    "\n",
    "You should aim to collect information from a diverse range of sources before providing the answer to the user. \n",
    "Once you have collected plenty of information to answer the user's question use the `final_answer` tool.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e4844344-5bc9-43d8-a57d-cc0521ef2120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi.\n",
      "\n",
      " You may use the following tools:\n",
      "{'type': 'function', 'function': {'name': 'browser', 'description': 'Search on DuckDuckGo browser', 'parameters': {'type': 'object', 'properties': {'q': {'description': None, 'type': 'string'}}, 'required': []}}}\n",
      "{'type': 'function', 'function': {'name': 'final_answer', 'description': \"Returns a natural language response to the user. There are 3 sections to be returned to the user:\\n- `answer`: the final natural language answer to the user's question, should provide as much context as possible.\\n- `add_tool`: additional information regarding which tool you used to get the answer.\\n- `add_source`: additional information regarding what's the source of the answer.\", 'parameters': {'type': 'object', 'properties': {'answer': {'description': None, 'type': 'string'}, 'add_tool': {'description': None, 'type': 'string'}, 'add_source': {'description': None, 'type': 'string'}}, 'required': ['add_tool', 'add_source']}}}\n"
     ]
    }
   ],
   "source": [
    "def get_tools(prompt:str, lst_tools:list[dict]):\n",
    "    str_tools = \"\\n\".join([str(tool) for tool in lst_tools])\n",
    "    return (f\"{prompt}\\n\\n You may use the following tools:\\n{str_tools}\")\n",
    "\n",
    "print( get_tools(prompt=\"hi.\", lst_tools=[tool_browser,tool_final_answer]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "43adda9d-b885-4440-8625-816b9334b417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-09-13T15:41:56.468554Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 34,\n",
      " 'eval_duration': 5619322000,\n",
      " 'load_duration': 21254740,\n",
      " 'message': {'content': '{\"name\":\"final_answer\", \"parameters\": {\"answer\": '\n",
      "                        '\"Hello! How can I assist you today?\", \"add_tool\": \"\", '\n",
      "                        '\"add_source\": \"\"}}',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'llama3.1',\n",
      " 'prompt_eval_count': 491,\n",
      " 'prompt_eval_duration': 49945632000,\n",
      " 'total_duration': 55607385321}\n"
     ]
    }
   ],
   "source": [
    "# LLM deciding what tool to use\n",
    "from pprint import pprint\n",
    "\n",
    "res = ollama.chat(model=llm,\n",
    "                  messages=[{\"role\":\"system\", \"content\":get_tools(prompt=prompt, lst_tools=[tool_browser,tool_final_answer])},\n",
    "                            {\"role\":\"user\", \"content\":\"hello\"},\n",
    "                           ], format=\"json\")\n",
    "\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4a2a288c-fe0a-43bb-af08-cebd7d12ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"browser\", \"parameters\": {\"q\":\"September 9, 2024 Death\"}}\n"
     ]
    }
   ],
   "source": [
    "# LLM deciding what tool to use\n",
    "res = ollama.chat(model=llm,\n",
    "                  messages=[{\"role\":\"system\", \"content\":get_tools(prompt=prompt, lst_tools=[tool_browser,tool_final_answer])},\n",
    "                            {\"role\":\"user\", \"content\":q},\n",
    "                           ], format=\"json\")\n",
    "\n",
    "print(res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6fa7b4d8-9486-47fb-a5fe-5ef9c35a3d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool output:\n",
      " Search in the United States of people who died on September 9, 2024 obituaries and condolences. Find an obituary, get service details, leave condolence messages or send flowers or gifts in memory of a loved one. Died September 9 2024. News Obituaries James Earl Jones (1931-2024), the voice of Darth Vader and Mufasa. by Linnea Crowther September 9, 2024. Events. Deaths. Sep 1 John Schultz, Australian Football HOF ruckman (Brownlow Medal 1960; All Australian 1961; Victoria 24 games; Footscray FC), dies at 85. Sep 2 Aleksandr Medved, Ukrainian freestyle wrestler (Olympic gold USSR heavyweight 1964, 68, 72; World C'ship gold x 7), dies at 86. September 9, 2024, 12:57 PM 0:49 A firefighter stands outside the FDNY Ten House on the 20th anniversary of the September 11th terrorist attacks on the World Trade Center in New York City, Sep. 11 ... Actor James Earl Jones, known for his booming voice and many memorable performances, died Monday morning, according to his agent Barry McPherson. ... Mon September 9, 2024 Link Copied!\n",
      "\n",
      "llm output:\n",
      " According to the provided text, James Earl Jones (1931-2024), the voice of Darth Vader and Mufasa, passed away on September 9, 2024.\n"
     ]
    }
   ],
   "source": [
    "# LLM with context\n",
    "context = browser(q=res[\"message\"][\"content\"][\"parameters\"][\"q\"])\n",
    "print(\"tool output:\\n\", context)\n",
    "\n",
    "res = ollama.chat(model=llm, \n",
    "                  messages=[{\"role\":\"system\", \"content\":\"Give the most accurate answer using the folling information:\\n\"+context},\n",
    "                            {\"role\":\"user\", \"content\":q}])\n",
    "print(\"\\nllm output:\\n\", res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0ff2b-28cf-43f2-beda-cd620c4f534a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3 - Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe47f093-1152-46a9-98c1-6bf60a725603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from\n",
      " {\"name\": \"browser\", \"parameters\": {\"q\": \"deaths on September 9, 2024\"}} \n",
      "to\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataStructure(tool_name='browser', tool_input={'q': 'deaths on September 9, 2024'}, tool_output=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel #this is the standard class\n",
    "import json\n",
    "\n",
    "# Taking for example the last LLM response, I want this structure:\n",
    "# {tool_name='final_answer', \n",
    "#  tool_input={'answer': \"Hello! It's nice to meet you.\", \n",
    "#               'add_tool': 'None', \n",
    "#               'add_source': 'User greeting'}, \n",
    "#  tool_output=None}\n",
    "\n",
    "class DataStructure(BaseModel):\n",
    "    tool_name: str  #<--must be a string\n",
    "    tool_input: dict #<--must be a dictionary\n",
    "    tool_output: str | None = None #can be a string or None\n",
    "    \n",
    "    ## function to create a new instance of the class with a specific input\n",
    "    @classmethod\n",
    "    def from_ollama(cls, res:dict): #<--return the class itself\n",
    "        try:\n",
    "            out = json.loads(res[\"message\"][\"content\"])\n",
    "            return cls(tool_name=out[\"name\"], tool_input=out[\"parameters\"])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error from ollama:\\n{res}\\n\")\n",
    "            raise e\n",
    "    \n",
    "    ## function to define how the class prints stuff\n",
    "    def __str__(self):\n",
    "        text = f'''Tool: {self.tool_name} \\nInput: {self.tool_input}'''\n",
    "        if self.tool_output is not None:\n",
    "            text += f\"\\nOutput: {self.tool_output}\"\n",
    "        return text\n",
    "\n",
    "# test\n",
    "data = DataStructure.from_ollama(res)\n",
    "print(\"from\\n\", res[\"message\"][\"content\"], \"\\nto\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3ab84e7f-ffe8-44e5-a896-72e2e9ecff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool: browser\n",
      "                 \n",
      "Input: {'q': 'deaths on September 9, 2024'}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d37c6-a49f-47e4-9e83-89e0a1e7b111",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4 - Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "05a313a5-eab2-4bfc-a96d-7fd0cd0cd870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Messages in Memory will have this structure:\n",
    "[{'role': 'assistant',\n",
    "  'content': '{\"name\": \"final_answer\", \"parameters\": {\"answer\": \"Hello! How can I assist you today?\", \"add_tool\": \"\", \"add_source\": \"\"}}'},\n",
    " {'role': 'user', 'content': None}]\n",
    "'''\n",
    "\n",
    "def create_memory(lst_data:list[DataStructure], user_q:str) -> list:\n",
    "    ## create\n",
    "    memory = []\n",
    "    for data in [data for data in lst_data if data.tool_output is not None]:\n",
    "        memory.extend([\n",
    "            ### assistant message\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps({\"name\":data.tool_name, \"parameters\":data.tool_input})},\n",
    "            ### user message\n",
    "            {\"role\":\"user\", \"content\":data.tool_output}\n",
    "        ])\n",
    "    \n",
    "    ## add a reminder of the original goal\n",
    "    if memory:\n",
    "        memory += [{\"role\": \"user\", \"content\": (f'''\n",
    "                This is just a reminder that my original query was `{user_q}`.\n",
    "                Only answer to the original query, and nothing else, but use the information I gave you. \n",
    "                Provide as much information as possible when you use the `final_answer` tool.\n",
    "                ''')}]\n",
    "    return memory\n",
    "\n",
    "create_memory(lst_data=[data], user_q=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8587b83a-c1aa-4b78-a23a-ba1db31571b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [{\"role\": \"user\", \"content\": \"hi there, how are you?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"I'm good, thanks!\"},\n",
    "                {\"role\": \"user\", \"content\": \"I have a question\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"tell me\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc2702-61d5-4ed5-b958-8d0c29645c18",
   "metadata": {},
   "source": [
    "### 5 - Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "623dcfac-fde3-41a2-bc74-9aa47d1d1b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': '\\n'\n",
      "             'You know everything, you must answer every question from the '\n",
      "             'user, you can use the list of tools provided to you.\\n'\n",
      "             'Your goal is to provide the user with the best possible answer, '\n",
      "             'including key information about the sources and tools used.\\n'\n",
      "             '\\n'\n",
      "             'Note, when using a tool, you provide the tool name and the '\n",
      "             'arguments to use in JSON format. \\n'\n",
      "             'For each call, you MUST ONLY use one tool AND the response '\n",
      "             'format must ALWAYS be in the pattern:\\n'\n",
      "             '```json\\n'\n",
      "             '{\"name\":\"<tool_name>\", \"parameters\": '\n",
      "             '{\"<tool_input_key>\":<tool_input_value>}}\\n'\n",
      "             '```\\n'\n",
      "             'Remember, do NOT use any tool with the same query more than '\n",
      "             'once.\\n'\n",
      "             \"Remember, if the user doesn't ask a specific question, you MUST \"\n",
      "             'use the `final_answer` tool directly.\\n'\n",
      "             '\\n'\n",
      "             'Every time the user asks a question, you take note of some '\n",
      "             'keywords in the memory.\\n'\n",
      "             \"Every time you find some information related to the user's \"\n",
      "             'question, you take note of some keywords in the memory.\\n'\n",
      "             '\\n'\n",
      "             'You should aim to collect information from a diverse range of '\n",
      "             'sources before providing the answer to the user. \\n'\n",
      "             'Once you have collected plenty of information to answer the '\n",
      "             \"user's question use the `final_answer` tool.\\n\"\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             ' You may use the following tools:\\n'\n",
      "             \"{'type': 'function', 'function': {'name': 'browser', \"\n",
      "             \"'description': 'Search on DuckDuckGo browser', 'parameters': \"\n",
      "             \"{'type': 'object', 'properties': {'q': {'description': None, \"\n",
      "             \"'type': 'string'}}, 'required': []}}}\\n\"\n",
      "             \"{'type': 'function', 'function': {'name': 'final_answer', \"\n",
      "             '\\'description\\': \"Returns a natural language response to the '\n",
      "             'user. There are 3 sections to be returned to the user:\\\\n- '\n",
      "             \"`answer`: the final natural language answer to the user's \"\n",
      "             'question, should provide as much context as possible.\\\\n- '\n",
      "             '`add_tool`: additional information regarding which tool you used '\n",
      "             'to get the answer.\\\\n- `add_source`: additional information '\n",
      "             'regarding what\\'s the source of the answer.\", \\'parameters\\': '\n",
      "             \"{'type': 'object', 'properties': {'answer': {'description': \"\n",
      "             \"None, 'type': 'string'}, 'add_tool': {'description': None, \"\n",
      "             \"'type': 'string'}, 'add_source': {'description': None, 'type': \"\n",
      "             \"'string'}}, 'required': ['add_tool', 'add_source']}}}\",\n",
      "  'role': 'system'},\n",
      " {'content': 'hi there, how are you?', 'role': 'user'},\n",
      " {'content': \"I'm good, thanks!\", 'role': 'assistant'},\n",
      " {'content': 'I have a question', 'role': 'user'},\n",
      " {'content': 'tell me', 'role': 'assistant'},\n",
      " {'content': 'who died on September 9, 2024?', 'role': 'user'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataStructure(tool_name='browser', tool_input={'q': 'celebrity deaths september 9 2024'}, tool_output=None)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def agent(user_q:str, chat_history:list[dict], lst_data:list[DataStructure]) -> DataStructure:\n",
    "    ## start memory\n",
    "    memory = create_memory(lst_data=lst_data, user_q=user_q)\n",
    "    \n",
    "    ## track used tools\n",
    "    if memory:\n",
    "        tools_used = [data.tool_name for data in lst_data]\n",
    "        lst_tools = []\n",
    "        if \"tool_browser\" in tools_used:\n",
    "            lst_tools = [tool_final_answer]\n",
    "            memory[-1][\"content\"] = \"You must now use the `final_answer` tool.\"\n",
    "        else:\n",
    "            lst_tools = [tool_browser, tool_final_answer]\n",
    "    else:\n",
    "        lst_tools = [tool_browser, tool_final_answer]\n",
    "        \n",
    "    ## messages\n",
    "    messages = [{\"role\":\"system\", \"content\":get_tools(prompt,lst_tools)},\n",
    "                *chat_history,\n",
    "                {\"role\":\"user\", \"content\":user_q},\n",
    "                *memory]\n",
    "    pprint(messages)\n",
    "    \n",
    "    ## output\n",
    "    res = ollama.chat(model=llm, messages=messages, format=\"json\")\n",
    "    return DataStructure.from_ollama(res)\n",
    "\n",
    "# test\n",
    "data = agent(user_q=q, chat_history=chat_history, lst_data=[])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab77b7-8380-4b4c-9914-c34c0404798f",
   "metadata": {},
   "source": [
    "### 6 - Graph Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9aad9-b664-4cc1-8085-26364bf041a1",
   "metadata": {},
   "source": [
    "##### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "377fdf1e-1ef3-4f53-8dc5-2471bb1802c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_q: str\n",
    "    chat_history: list[BaseMessage]\n",
    "    lst_data: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "    output: dict[str, Union[str, List[str]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "415d4025-9cde-4871-8200-85a16b64e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "g = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0581b8-ca39-4e08-b279-f69b9fdedb2c",
   "metadata": {},
   "source": [
    "##### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "982a35b4-2523-44b7-99e8-8537a19ce0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(state: TypedDict):\n",
    "    print(\"--- starting ---\")\n",
    "    chat_history = state[\"chat_history\"]\n",
    "    out = agent(user_q=state[\"user_q\"], chat_history=chat_history, lst_data=state[\"lst_data\"])\n",
    "    return {\"agent\":[out]}\n",
    "    \n",
    "g.add_node(\"agent\", start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7d66fa3e-7de0-4ea7-90e7-26665cb9b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_tool(state: TypedDict):\n",
    "    print(\"--- using tool ---\")\n",
    "    tool_name = state[\"lst_data\"][-1].tool_name\n",
    "    tool_args = state[\"lst_data\"][-1].tool_input\n",
    "    print(f\"run_tool | {tool_name}.invoke(input={tool_args})\")\n",
    "    out = tool_str_to_func[tool_name](**tool_args)\n",
    "    action_out = DataStracture(tool_name=tool_name, tool_input=tool_args, tool_output=str(out))\n",
    "    return {\"output\":out} if tool_name == \"final_answer\" else {\"lst_data\":[action_out]}\n",
    "\n",
    "g.add_node(\"browser\", use_tool)\n",
    "g.add_node(\"final_answer\", use_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b55681-db5b-4650-955a-875d1c7da163",
   "metadata": {},
   "source": [
    "##### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fe58b028-e548-430c-a009-608b144aae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_path(state: TypedDict):\n",
    "    print(\"--- moving path ---\")\n",
    "    return state[\"lst_data\"][-1].tool_name if isinstance(state[\"lst_data\"], list) else \"final_answer\"\n",
    "\n",
    "g.set_entry_point(\"agent\")  #<--user query\n",
    "g.add_conditional_edges(source=\"agent\", path=move_path) #<--agent starts\n",
    "\n",
    "# create edges from each tool back to the oracle\n",
    "for tool_obj in [tool_browser, tool_final_answer]:\n",
    "    tool_name = tool_obj[\"function\"][\"name\"]\n",
    "    if tool_name != \"final_answer\":\n",
    "        g.add_edge(tool_name, \"agent\")\n",
    "\n",
    "g.add_edge(\"final_answer\", END)\n",
    "graph = g.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb86f8a-f7c6-41d5-b314-c07c7b385ba9",
   "metadata": {},
   "source": [
    "##### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5ccd3876-579f-463c-8a8f-29baea3c5574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAFNATUDASIAAhEBAxEB/8QAHQABAQADAAMBAQAAAAAAAAAAAAYEBQcCAwgBCf/EAFEQAAEEAQIDAwULBwoDBwUAAAEAAgMEBQYRBxIhEzFBCBQXIlYVFjI2UWFxlJXR0iMzQlJ0s9MkN1RicnWBkbK0GFWhCSUmRWSx8DRjgpPB/8QAGwEBAAMBAQEBAAAAAAAAAAAAAAECAwUEBgf/xAA2EQEAAQICBwUHAwQDAAAAAAAAAQIDEVESFCExUnGRBDNBsdEFEyJhgZLBMmKhFSPh8EPC8f/aAAwDAQACEQMRAD8A/qmiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIimrFq7qmzNVx1mTHY2B5inyETWmSZ46OZCTuAAdw55B6ghvUFzdKKNL5QmIb63frUGB9mxFXae50rw0H/NYPvqwn/OKH1pn3rEp6A07TeZRiKtiySC61cZ5xO4/1pZOZ58e8+JWZ71sL/yih9WZ9y0wsx4zPSPU2Pz31YT/AJxQ+tM+9fvvqwp/84ofWmfenvWwv/KKH1Zn3J71sMP/ACih9WZ9yf2fn/BsZ1a5Bdj7SvPHPH+tE8OH+YXuU7Y4f6flk7aDGQ462N+W3j2+bTN//NmxP0HcfMlHIXcJfhxmWmdchn9Wpk3Na0yO2/NyhoDQ/vILQGu2I2aQA6JopqjG3P0n/dphkokRFggREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERBotcZOfE6Xuy1HiK7LyVa8hG4ZNK9sUbv8HPaVs8XjK+GxtWhUZ2datG2KNu++zQNhufE/OtHxGYRpOxZAc4UZ62QcGt5iWQTxzO2Hj6sZVKHBwBBBB6gjxXonuY5z5R/lPg5XxE8onC8PtdVtHtwGpdU6gkoDJy1NOY7zp1aqZDGJZN3N6FwI2bzH5uo3j8D5SOcyHlPao4dz6PzM2Fx7KkVe3UosPYPk5i+xZeZekLthyFrd9gdwtR5WPDLVuu87jrGidCSz6mrVGtx2u6GoY8bPjJe1JdFLGdnTQ8vrbDm+E4AA9Tn4vRnEnQPlJWtTwabr6uwuqMVi8flcrDkYqhx8sHqTTGJ/rSNILnhrB16DovOhYyeUnhcfxIqaPzGmNV6efeyD8XQzWUxfZY27ZBdysimDyTz8pLSWgOHULWYnyrcRqPVWpcBg9Ea1zlrT2Rt4y/YoY2F9dksDXE7SGYA8/I4MHwieXdreYb/N2P8mXic3Umj8lldBRZTVeC1lXzeW1vLqKOWbL1GWi7khge78k1sbmktdy/mhsCXbL6b8m7h9ntAy8VXZ7H+Ye7mu8nmaB7aOTt6coi7OT1HHl35Xeq7Zw26gINZ5I3lAZvj1oCPJZ7TeQxl8Ome7JCoIcbZHnErGsru7Rz3FjWta7mA9YHqV2XUmHGewdujzBkkjQ6GU7/AJKVpDo5Bt4te1rh87QuI+SLpLXnCnSTuHmqNKxUsThZLT6Wo4MnFMzICWy+VoEDfXj6SHq7buHTqu85C9FjKFm5OS2CvE6aQgb7NaCT/wBAr0TMVRNO9MMPS2Y98OmsVky0MdcqxzuaP0S5oJH+BJC2i0GgcfLi9FYOrO0tsMpx9q0jYh5aC4bfMSVv1a7FMXKop3YyTvERFkgREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREHjIxsrHMe0PY4EOa4bgj5CpbG3maIEOJyUrYcYCIsffld6nL3NhkcejXj4LST642/S3Cq1654I7UL4Zo2SxSNLXxvaHNcD3gg94WtFcRE01bYlMS9iKYHD6hVP/AHbcyWHj337ClceIR8zY3czGj5mgBePvIn9qc9/++L+Er6Fud1fWPTFOEZqlFyuljcrPxVzGnn6pzPudVwtG/ERNF2nayz22P3PZ/B5YI9unfzdT4Vg0ROCD76c8fmM8X8JPd2+P+JMIzUz3tjaXOIa1o3JJ2AClrMzNeSMrVdpNPRyNksWx8C45rg5sUR/SZuAXv+CQOQc27+T2x8PsbK5rsjPfzXKdxHkbb5Iv8Ytww/4tKpWtDGhrQGtA2AA2ACaVFvbROM9MP9+hsjc/URF51RERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQc+xhb6f9SDc83vYxe4+bzvIfP9Ph/ifDoK59jN/T/qT4O3vYxfgN//AKvIf47fT079vFdBQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREHPcYB/xBalPM0n3r4r1duo/leQ693/zYroS55i9v+ILUvU7+9fFbjb/1eQ8f/n/VdDQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERARR02r8rlHyPwVCnLRa5zGW71h7O2IOxLGNYfU33AcSN9twC0hx9Pu5rD+g4P61N/DXrjstzxwj6wnBbooj3c1h/QcH9am/hp7uaw/oOD+tTfw1Oq15x1gwfHGk/L3yue8oibFVuFVhuospHV04cdLmA0wTQ2LDnOe/zfflHbnfceqGE+JX38vmnCeT9LgfKEyvFuvQw3uzfqdj5p28gihmcOWSw0iPfnewcp/tPP6XTr/u5rD+g4P61N/DTVa846wYLdFEe7msP6Dg/rU38NPdzWH9Bwf1qb+Gmq15x1gwW6KSoasyVS3XhztGrXhsSNhjt0p3SMbI4gNa9rmNLeYnYEEjfYHbcb1qwuW6rc4VGGAiIskCIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIg5zw3PNw80u47buxdVx2G3UxNJVGpvhr/ADc6W/uqr+5aqRdm/wB7XznzTO+RERYoEREBFgjOY85s4cXYDlRXFs0hIO1EJdyCQt7w0uBAPcSD8hWcgn9dnbT247xcpkfMfOYl0Nc7158XT+2U/wDcxLoiz7R3VHOfwt4CIi8CoiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIi/HODGlziA0Dck+CDnPDX+bnS391Vf3LVSKb4a/zc6W/uqr+5aqRdm/3tfOfNM75fIeO1lqE690VrTAXNR+8/UeqnYkSZ3UBsRXYJO3H5Oh2fLAxro92PDw/Zg5mnm3WK3M6h1RrmvWOotVu17BrxsGQ09XsWIsdBh47HOHFjNoxH2DY39pvzPc7lPMHFq75F5OHDqDKMyEenAy1FcGQrltywGVbAkEnPAztOWEl43IjDQ7qCCCQua5PyddX2OIdrKYmxiNMVJs17p+62LzWVFkxmftZGOpOk82LpBzNcfgnnceXwXj0ZQk8KeLnF6PUWqNPX3UcnXzdylR7XVUtarRFecxthmxzaj45PVaC7neXO59wW7gDZax938jU8oDPjWGosdd0jKbWIrUcnJHVrSR4yCcgxjpIxzhsWP3b1cQ0Fzie3ZDgHoLJ6ufqafANGYksR25ZIbU8UU07CCyWSFjxG94IB5nNJ3HetxZ4Y6at09W1ZcbzwarDhmWdvKPOuaBsB6h27PybWt9Tl7t+/qp0ZHFsLpyvqvyq6mbs38tXtTaJx2W7GplbEMJk86eDGY2vDXRdATGQWEucSN3En6RUbmeD+ks9lcDk7mKJyGCjbDQtQWpoZI42lpEbnMeDIzdrTyv5huO7qVZK8RgJ7XnxdP7ZT/3MS6Iud68+Lp/bKf+5iXRFXtHdUc5/wCq3gIiLwKiIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiLwmmjrxmSWRsUY23c8gAeHeg80U/79aVmXs8ZDZzTo8l7l2TQjDm1ZQN3ukc4tAawfCIJO/qgF3RK/vmyElOWY4/Cwx2pDYrMDrck8A3EYEn5MROPRzvVeB8EE/CQUCnq+usTlIa8uGlfqCOzFNNBNiW9vBJ2ZLXN7cfkmu52lgDnjcg/quIUNEUa82KtXZrWayWMdO+tfyMgdKx03R52aGsHq+qNmjZu4G25330UTIImRRMbHGxoa1jBsGgdwA8AgnmSamzETT2VbT8FjHO37RwsW6tt3weg/JFrB1PV3MenQDc/r9C0Mgx4zMljUBnoMx9qLIyc9awwdXOfWG0PM89XEMG46dG9FRr+fOZ/7Q7inZ4qP4f4jhliaGpfdI4sUbtuSy5s3Py7FzCxpA7+ceqR17kH1/wANf5udLf3VV/ctVIprC4rU2kMLTxs2LZnvN4WMbYxkrIh0A3byTSAgA7gHmdu0DfruFle62f8AY3J/Wqf8ddquIuVzXTVGEznEecrTGM4t2i0nutn/AGNyf1qn/HT3Wz/sbk/rVP8Ajqnu/wB0fdT6mDdotJ7rZ/2Nyf1qn/HWHiNV5fOY2vfq6NzHm87eZnbS1Ynjrt1Y+YOaencQE93+6Pup9TBTotJ7rZ/2Nyf1qn/HT3Wz/sbk/rVP+Onu/wB0fdT6mDD4k2H1NIWJ468tuSKxVe2vBy9pKRYjIa3mIbue4bkDr1IVti9S43L3Z6MFpgyVaGKexQkcG2IGSgmMvj72g7OAPcSxw33adpqLFZbU89eK/jHYbGwzR2JRPOx80zmOa9jGiNzmhvM0cxLu4coaebdtRnsBR1LjJcfkYXTVpeUkMkfE8Frg9pa9hDmkOa0gtIIIBXm7RVGjTRjjMYznvw9Cd2DYotDZgz2OsGSlLDl4Z70ZfBeeIDVrEbSdm9jDzlp2c1rwN+oLx0I92N1TRyMra7zJj7r5poI6d9nYzSmL4bo2u/ON2IdzM3GxHVeFVuEREBERAREQEREBERAREQEREBERARanJ6oxuKyEWOmsB+Tmglsw0IQXzyxxjdzmsHXbuG56EkDvKwW29RZuP+T04cDTs4wvjnuPEt2tbcfVY+BoMZaxvUkSndx22AG5ChmmjrxPlle2OJjS5z3nYNA6kk+AWgyGta0LsrXxtO5nsljXQMno0GND95diwB8jmR9GnnPr9G7E97QfyTQuOyLJxmu11CLFaGvYhybu1rSdmQ4P83/NNcXjmJawEkDwa0CiA2Gw6BBP3KupMo+/Ay3UwdcWIvNLVUeczyQjrJzte0MY5x9Ubc4A69Sdm+btEYixPYlvQOy7pbjL7WZOR1mOCVg2jMLHkti5e8cgHXc95JW+RAREQFp9TatxWkKcdjJ2uyMzxFBBGx0k9iQ9zIo2gukd8zQTtue4FTFriFe1Zamxug4K+RMb3RWNRW2l2MqPB2c1vK5rrUgO4LI3BoIcHyRuAB2ulOHtHTd2XK2bFjOajnZ2c+ZyLg+Yt33McYGzYYtwD2cYa3cbkFxLiGoZiNS8RD2mdM2ldOu2LcLUnHn1pvQ/yqdhIiHyxQuJPjKQXMXrxvk98P8ACcScdrrF6apYrP0Me7GQuoxNhhERADT2bQGh7WBzA4bHkeWncBvL0VEBERAREQFoNJSlozNJz8rM6nkpmmbKsA5xJtOBC4AB8LBMI2nw7MtJJaVv1OUXCrrzLQGTLy+dUq9kCcc1CLldIwthP6Mh9Uvb8nIR4oKNERAREQFj28fVvmF1mvFO6B/aROkYCY37FvM0+B2c4bjrsT8qyEQTMOOy2lKrG0JZs5i6dGRrKNmQy5CeZp5ow2zLIGu3buz8r1JDSZPhE7TFagpZeR8EUojvRQxTT0ZSBYrtkBLO0Zvu3fZw+TdrgD0K2S1ubwNfOVZIny2KczuTlt0pTDOzkeHt2e3rtzAbtO7XDdrg5pIIbJFomZe9irggy8bJIrVySOnaowPMccXJzs84337N3R7ebfkcWt6tc8MW9QEREBERAREQEREBEWi1A27lbAw1YTVatmvI6zlK1psc1YbtDWsbsXczwX7OGwbykg82wQed/VNevbZUpwTZi2LcVSzBjzG51Pnbz9pNzOaGNaz1jueYgt5WuLmg+mtisxkJadnK5HzN1eaZ5oYt35CZjukYle9vO4tb19XkBceoIAW6r04KYkEEMcPaPMj+zaG8zj3uO3eT8q9yDAweBx2mcVXxuKpQ0KNdvLHBAwNa0bkn6SSSST1JJJ6krPREBERARFFZrXti9lLWA0hWgzGcrkMt2Z3ubRxpOx/LyNBLpNjzCBnrn1eYxNcHoNzqvWeJ0XSisZOyWSWJOxq1IWGWxbl2JEcMTd3SP2BOzQdgCTsASJf3q5viTtLq9r8Np93Vmla8wMkw/wDXTxuIeD03gjPZ/CDnzNOw3mlNCQaetz5S7cnzmorTOSxlrm3Py779lEwerDED3MYOuwLi927jUIPXWrRU68VevEyCCJgZHFG0NaxoGwAA6AAeC9iIgIiICIiAiIgKcy0zauudPPdJlibFe3VENbY0dz2UnPOPB4ERbG7/AO5ID3hUandUTGDNaTIdlgH5J8Zbjmh0Dt6lg/yrfuhBAII69r2I7iUFEiIgIiICIiAiIg/CAQQRuD4Kbjp+8eCJlGD/AMOV4Ya0OMpwbuqflOUvZ1/NNa9u7APUbF6oPwRSogIpiua+islXpb0MfgLrxFUjL3tkbce57zGAd2cjh8EAt2c3YB3OOWnQEREBEWkzGttPaftCtk85j6Fkjm7GxZYx+3y8pO+yvTRVXOFMYynDFu0Ut6VNHe1GJ+uR/enpU0d7UYn65H9611a9wT0lOjOSpUNqvUOB0Bq+vns7NicFQvUHUp8/lMnHVDXxSB8FblkcA7mEtl+46jszv8IbbD0qaO9qMT9cj+9fz08tnyb8BqXiNR1poHLY64zPXmRZqhBaY4wTPf61obHfkduS/wDVIJ7jsGrXuCekmjOT+lmPyFXLUK16jZhu0rMTZoLNeQSRyxuALXtcOjmkEEEdCCshQ2n9c6E01gcbiKWpcTHTx9aOpAzzyP1Y2NDWjv8AkAWw9KmjvajE/XI/vTVr3BPSTRnJUopb0qaO9qMT9cj+9PSpo72oxP1yP701a9wT0k0ZyVKxcnlKeFoT3shaho0oGl8tixIGRxt+Vzj0AUVqXjjpDT2MNmLLV8vZe8RQ0sfPG+SV532BcXBjB06ve5rR4nqN8HR9WlxAyUOYzucxeeyVN7bFbCYu22ejjHAnleBsHSyjf89IBtt+TZHu7mrVZu0RjVTMRylGEsrtc7xR27A3NLaPeDvOC+vlMi35GjYOqREfp79s7c7CHlDnW2FwtDTmLrY3GVIqNCs3kirwN5WsHf3fOSST4kklZqLFAiIgIiICLVZrVeF02Y25XLUsa6QbsbasNjLx47Anc/4LV+lTR3tRifrkf3ramzdrjGmmZjlKcJlUopb0qaO9qMT9cj+9PSpo72oxP1yP71bVr3BPSU6M5KlFLelTR3tRifrkf3p6VNHe1GJ+uR/emrXuCekmjOSpXMeIvGHQWltT4DF5rW1TEZODJjtqcWWghMXNUmc3zxjngiAtII3HV5hPiFS+lTR3tRifrkf3r4M8uTgdieKvGXSepdKZvHTNz0sWOzU0dljm1OQANsv69GdmOX6WDxcE1a9wT0k0Zyf0Nweexmp8XBk8PkamWxs+5huUZ2zQybEtPK9pIOxBHQ94Kz1z3Rup+Hmg9KYnTuI1FiK+MxlZlWCMW4/gtG2569Se8nxJJW59KmjvajE/XI/vTVr3BPSTRnJUopb0qaO9qMT9cj+9PSpo72oxP1yP701a9wT0k0ZyVKKW9KmjvajE/XI/vT0qaO9qMT9cj+9NWvcE9JNGclSimYOJmkbMrY49TYl73EADzyPqe4eKplnXbrt/riY5omJjeIiLNDCzNCTJ4m3VgsmlYlic2G02NkjoJNvUkDXgtJa7ZwDgRuBusbS+ajz2GisMldNJG+StO91d0BM0T3Ryfk3Elo52O26kbbEEggnbKew9h1fV+oMfJav2S9tfIRtsQ7V4GPaYuyhkHwvWrue5p6tMoPc4ABQoiIMLNXHY7D3rTAC+CCSVoPytaSP/AGUjpKrHX0/SkA5p7MLJ55ndXzSOaC57iepJP+Xd3BU+qvixmP2Ob/QVPaZ+LmK/ZIv9AXQsbLU81vBskRFdUREQEREBERAU/rQijixlohyXaEkckMzejgOdoc3f9VzSQR3dfmCoFO8QvifkPoZ+8atrO25THzWp3w6KiIuMqIiICIiDnujSL+Plyso57t2eV0szuri0SODGb+DWtAAA6eO25K36ntAfFSp/bm/evVCuxe2XKo+aZ3iIiyQIiICIiAiIgIiICIiDxkiZNG6ORjXscNnNcNwR84Xo4dTObRytAOcYMdkH1YGuO/JHyMe1gJ8G8+w+QADuAWSsHh1+e1T/AHw7/bwJXttVfTzWjdKxREXMVFO3ZfN9fYlpnyZbZx9pogjZzUt2SQnnkP6MmziG/KO0+QKiU5nZey1bpgdvk2CR9iPsqjN6r/yRdvYPgBy+qf1jt4oKNERBq9VfFjMfsc3+gqe0z8XMV+yRf6AqHVXxYzH7HN/oKntM/FzFfskX+gLo2e5nn+FvBsl8ucCePGp8Jww4cT6y09cs4HMvjxrdWy5YWp3WZZHNidYiI5msc7Zofzu26bgbr6jXzZo7gFxBi0ZofQmpMhptmkdPW61+xPjHWH3Lr4Je2ZDyvY1jGdptu/clwb0a3fYRVjjsVV2G4/5DJWNZ37OlY8dpDSOQyFLKZubJ7vLarHOL4YBFu/fZoILm7c3Qv2KxqPlA6jD8Q3LcPJcSdRUrFnTrTlWSutzRwGdlawGx/wAne9jSRt2gGxHeNltsBwWmOheJemM5ZgNXV2WytpklNznOir2xs3fmaNpGjrsNxuB1K1Gm+FevctqbQ1jW+QwDsVoxr5KgwxmdNkbBgMDJZhI0CINY5zuVpfu49+yj4hsdOeUtpzUWc4d4tkbop9Y4d+UicZN21HBnM2GQ7AczuSyAenWu4bdek9T8qHJagsaaqYDRLL93UFe7kKEVzMsptsU4LHZRuje+Mh8sjdpBEPgtcCX7dVr7XkiRt0HxDxFHLCrls3lDkMJeBcPcqNkjpYIGnbcNa+awDtv6szu9UHF/gvm9UaDxmiNM43Ssun6uNbRgnznbtt42VjQyKxWdG0jmY0AgeqdwPW23Cj4hhcQ/KxxujNWZjB06mFuS4NrPdL3U1LVxkpkdGJOyrRyAmZwa4bk8jdzy7kg7bSn5QmQ1bq7G4TRmkhnIshpynqWO/dyQpxRwTySN5JB2TyHjkGwG+5LgeUN3OsocH9f8PNQ5uzpC/pvPUs8ILFx2qWzNmr3WQMhknZ2TXdo2QRteWOLdj3O2V5itAZGlxqyesJJaYxtrTtPEshiLhI2aKeeRx5eXYM2laB6xO4PTxM/EL9TvEL4n5D6GfvGqiU7xC+J+Q+hn7xq9Vjvaecea1O+HRURFxlRERAREQc70B8VKn9ub969UKntAfFSp/bm/evVCuze72rnKZ3y+atG6h15qfifxTzOQx8zo9NSPo43GV9Tyw0w8VonCN0TYOVxe2V0nbPDix3K0N9XmWx4fcbszPp7h3gdP6Zt6jyea0t7uNnzeeHPGxj42OE85hJeT2o9drNydvVA3cOgaM4d5LTuY4l27M9V8epsp57TET3Exs80hh2k3aNjzRuPTmGxHXfopXhLwRzug83w/uZC3j5otP6Mk07aFaSRzn2HTV3h0fMwbx7Qu6nY7ker37eXCUMqXj/ct8KsXrLGacpt7azNUv185nYcbDj5YZHxSNfO9rg78pG4N5W9eh6KcucaH8ScPwcz+FmtYeLI6ydjshTguB7XdnXuNkic+M8s0ZfG1wPVrgGnb5Met5PWrMLFpO9UdpzMZDBZjN3vc3KyzCnIy9YdJFKHCIkTRNO3VhHrvAcOhPsx3k/60xOj8XBFktPyagw+tJtU1Hcs0dOzHMJe0ie0AuiINiUN25xsxnXqQI+IfRa4nxa8o2Xg/qbzfMYHHjAB8IN12oK8d6Vjy0Okhokc8jWFxB9YH1XEAjqqyxxz0vWnkhfBqUvjcWO5NKZVzdwduhFYgj5wdlyHWnADVmt8fxCjwc2nDjdcSx5OLM5qvZZlK7RFF2dV0RYC1gMYAJcHMD3bxkjZWqnZ8Iu9WcdM9iNT63xOE0QM5FpKrBdvWpMs2t2kUkJl5Y2mNxdIA1+zTs07fCBIC8Y/KEs6g1th8BpXTDcy29hqedfPdykdKQVbBPKYY3Nd2xaAS7YtA6DfcrPq8L827UPFPKWJcfGdW4ynVqxRTPd2MsdWSJ/OSwerzPGxG5IG5APRQmq+AWt9R6E0RpGP3pwRYLF42szURdY908bagDBNLUcGAODgwBoJj8ebffYROkKXN+VDidJ4LL3s/jH461jNUjTU1Nk/aHlJbJ51vyA8nmxM5G3QNLd/FeWufKcxeh8hqiCfHxSQYq/Sw1W3NkY68Vy/PCZnxOfIAyFkUfI50hcfhEcu4AOblfJ5xee41ZfWeRe21i8jhDjpcU7flfYc10Mk5HduaxEQ8di75lKUPJfyeB4TadxGOztexrTBZ52oosrkWOlgvWC6RvJOB63K6F7YyW9Rygjfbq+IeDfLFos0hrHJOw1HIZfTcVOzJSwWdhyNa1DYnEIMdmNnR7XE7scwH4Pg7cdu0flM9l8ZLY1Bg4NP2jKRFVhv+dkxcrSHPcGNDX7lwLRzAcvRx3XNNYcOtd8RuE+osBmY9K4vMXrFR1RuKknNdkcU8Ur+0ldGHOceR22zABuB867QpjHxBYPDr89qn++Hf7eBZyweHX57VP98O/wBvAr1d1X9PNaN0rFERcxUU7qGXk1NpVvb5OPntTjs6TN68n8nkO1k+DBtu3+uGBUSnNRS9nqbSrPOMlFz2ph2dNm9eX+TSnawfBg23b/XDAgo0REGr1V8WMx+xzf6Cp7TPxcxX7JF/oCqczTdkcReqMID54JIgT4FzSP8A+qQ0lcjnwNODfks1YWQWK7uj4ZGtAc1wPUHf5uo2I6ELoWNtqea3g3CIiuqIiICIiAiIgKd4hfE/IfQz941USn9ZhuQxoxEThJfvSRxxQN6u5edpc8jwa0Akk9PDfchbWe8pn5rU74dDREXGVEREBERBzvQHxUqf25v3r1Qqf0eG46jLiJnBl6lNK2SF3R3KZHFjwD3tc0ggjp3jfcFUC7N7vKp+aZ3iIixQIiICIiAiIgIiICIiAsHh1+e1T/fDv9vAsuaaOvE6WWRsUbRu57yAAPnJXp4dwONHKX+VzYMjffagLgQXR8jGNdsQDs7k3HzEHxSvZaqnl5rRulWIiLmKindQziPU+lo/OMlEZLE+0dRm9eXavIdrB8Gjvb/XDVRKdzsxbqzTMQs5GLmfYcYqse9eXaI9J3fogb7t+VwQUSIiAtNmNF6f1DYE+UweNyU4HKJbdSOV4HybuBOy3KK1NdVE40zhJuS3os0Z7JYT7Pi/Cnos0Z7JYT7Pi/CqlFtrF7jnrK2lOaW9FmjPZLCfZ8X4U9FmjPZLCfZ8X4VUomsXuOesmlOaW9FmjPZLCfZ8X4U9FmjPZLCfZ8X4VUomsXuOesmlOaW9FmjPZLCfZ8X4U9FmjPZLCfZ8X4VUomsXuOesmlOaW9FmjPZLCfZ8X4VtcLpXC6b7T3JxFHGGTo806zIub6eUDdbRFWq9drjCqqZjmjGZERFigREQEREGrzWl8NqMRjLYmjk+z+B55WZLy/RzA7LVeizRnslhPs+L8KqUW1N67RGFNUxHNOMwlvRZoz2Swn2fF+FPRZoz2Swn2fF+FVKK2sXuOesp0pzS3os0Z7JYT7Pi/Cnos0Z7JYT7Pi/CqlE1i9xz1k0pzS3os0Z7JYT7Pi/CofiRw60tS1Bw6ZX09iqsdjUXY2GRU4miePzC47keNhzN5msdt16sB26bjsK59xbc6C5oG0DsyDU9YP7+6SGeEd39aVvemsXuOesmlObceizRnslhPs+L8KeizRnslhPs+L8KqUTWL3HPWTSnNLeizRnslhPs+L8KeizRnslhPs+L8KqUTWL3HPWTSnNLeizRnslhPs+L8KeizRnslhPs+L8KqUTWL3HPWTSnNN1uGukaczZYNL4aGVp3a9lCIEHv6HlVIiLOu5Xc/XMzzRMzO8REWaBTmUn313p+uLeQical2fzeGPerMGmBpMrvBzTIOQeO7/1VRqcM7Z+IYhbayDX1MUXvqhm1N4lm2a8u8ZR2DgB4Nc79ZBRoiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICg+ONOefhllblWA2reIfXzcELQS6R9OeOyGN27y7seXbx5tj0KvEQemnbhyFSC1WkbNXnY2WORvc5pG4I+kFe5c84fN94GTfoOy0soRNfY09YPwJKfMSag/VfW3awN8YuyILi2Tk6GgIiICIiAiIgIiICn9Oze6Gb1BdbLlBG2wyk2vdZyQNMTd3PgG25a4yEFx7yzp0AJ2OcyU2Kx0k9ajLkrW7WRVIXNa6RxIHwnEAAd5PgASAT0P5gMQMBhaWOFu3kPNohGbd+YyzzEd73uPe4ncnYAddgANgA2CIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIg1WpNNUdVY3zO8x+zZGzQzROLJa8rerZI3jq1wPiPnB3BIOg0/qu5h8pW03quWJmVnLmY7JtbyQZYNbzEAdzLAaHOdD4hrns3a14jtFgZzB0dSYmxjclXFmnYaA9hcWkEEFrmuaQWuaQHNc0hzSAQQQCgz0Xxn5TnlkW/Jp5NE07sOqNWsnqWY7ri0vr0e0D3x3W8uxmexpYOTlJZL2h7MhnafW+lNS0dZaYxOexkna4/J1Y7cD/Ese0OG/wAh2PUeBQbVERAREQFgZrLx4Sg+0+CzbILWsr04TLLI5zmsaA0dw5nN3cdmtG7nFrQSPlji55fGJ4P+UnDoTJ0op9JwVo4snlIQ51inbk2c1+wJD4mtLQ9obzjmJBJZyO+ldJUorlKjnp7lPL5O5SjacnQJ82liJdI3sBzOAjPP0cCS4BhcXbDYMihgi7KOymTbWtZKN00VSZkW3m1d7m7Rt3JO7gxhef0nD5GtA3KIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIg5bxP8mLhjxiuS3tU6RpXcpK0NdkouaCy7Zoa0ukYQXbAADm32AA8FJab1BjODej6OhtARyX8ZijLFFkcvMZmM5pHPLGBvKZA0uLQd2jZo6u6lWnGzU0uPxlLCVZDHNlC8zPadi2uzbnAPgXF7G/QXfIuRtaGNDWgNaBsABsAF9T7L9m0XaPf3oxid0fk3N3NxB1jO8u98stfc78tenXDR9HPG4/5lev386x9rbv1Sn/AWpRfTR2Xs8f8AHT9seiNKW3GutYg7++y4fmNSpt+5W7wfGLUWLlY3Jsr5uruA4sjEFgDxIIPI76Nm/wBpRqmNAa8rcQcdkblStLVjpZCfHlsxBLnREAu6eBPcsbnZey1/BVbp25REeRpS6NpnySeC2rs5ktcTYJ+qMjl7016aXM2JJeyle8udGYtw0cpOwDgTsB1Pee/4/H1cTQrUaNaGnSrRNhgrV4xHHFG0ANY1o6NaAAAB0AC4Bw/1LJpXV1J3MRQyUrKdqPw5nHlik+kPIb/Zed+4L6HXxPb+x6nd0Y20zthYREXMQIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIg4bxqDxr2iXb9m7Gep8m4ldzf+7FFrsvGHSE+oMTVyVCF0+QxbnuELAS6WF4HasaB3u9VjgPEs2HUrjEUrJ42yRuD2OG7XDuIX6D7Ku03ey0xG+nZJVm+HtN4nO67bPlXZzTOK4gSZd0bbuTzlqDJ1pxNs2BtcMLOQgcoaAQQfm2F/mdC4rVmqOO1/LQPnvYmCGxSljnkYK8wpF3aMAcBzbxs6kHoNu7dfRc2itPWM03MS4HGSZZpDhffTjM4I7j2nLzf9VkjTuJD8k8YymH5MBt53m7N7QDeUCXp6/qkj1t+nRVo9n4RhVOP52TGPPaq+ZZsnjtc6j4Z4/iNk+TTFnSEWQYLdp1eC5kSWhxkeHN3cGesAT3n+tseheSjFUg4fZmOg9slFmfvNrvbJzh0Yc3lIdueYbbdd+q6he0Zp/J4ypjbmCxlvHVABXqT043xQgDYBjCNm7DoNgsrD4LG6equrYrH1cbWc8yuhpwNiYXnvcQ0AbnxK3tdlqt3YuVTE+e6PQe68HuiibFv2zp4Wx7d/OZGhv/XZfVy4Bw00tJqnVNa09hOLxUonlkI6PnbsY4wfEtOzz8nK39Zd/Xz3ty7TXcotxvpxx+v/AIv4CIi+aQIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgLnesODtTOXJshibXuPflcXys7ISV5nnvc5m4IcT3uaRuSSQ4ldEReix2i72erTtVYSlwSbhFrGFxDYMTYHg9l2Ru/wBIMXT/ADK9foo1n/QcZ9oO/hLv6Lrf1rtWUdP8mzJwAcJ9ZE7eZYsfOcg7b90t3hOB+RsytfncpDWgBBNbF7vc/wCYyvaNh9DN/kIXZEVK/bHaq4wiYjlHrifRiYrE08HjoKNCuyrUhbyxxRjYDrufpJJJJPUkknqVloi4szNU4zvQIiKAREQf/9k=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(Image(\n",
    "    graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf810a-4da2-46a3-91b5-87693c58c658",
   "metadata": {},
   "source": [
    "##### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8dd7ff20-c631-4848-ab01-ab316b7fe555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- starting ---\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Must write to at least one of ['user_q', 'chat_history', 'lst_data', 'output']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_q\u001b[39m\u001b[38;5;124m\"\u001b[39m:q, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m:[]})\n\u001b[1;32m      2\u001b[0m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1468\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1467\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1470\u001b[0m     config,\n\u001b[1;32m   1471\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1472\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1473\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1474\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1475\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1477\u001b[0m ):\n\u001b[1;32m   1478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1479\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1221\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1216\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1217\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1218\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1219\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1220\u001b[0m ):\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1222\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1223\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1224\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1225\u001b[0m     ):\n\u001b[1;32m   1226\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1228\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/runner.py:87\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, timeout, retry_policy)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m _panic_or_proceed(all_futures)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/runner.py:190\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls)\u001b[0m\n\u001b[1;32m    188\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/executor.py:59\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m         task\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/retry.py:26\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     24\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/utils/runnable.py:343\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/utils/runnable.py:121\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    120\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m--> 121\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    123\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/write.py:86\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m write\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     85\u001b[0m     ]\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_write(\n\u001b[1;32m     87\u001b[0m         config,\n\u001b[1;32m     88\u001b[0m         writes,\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_at_least_one_of \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/write.py:139\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[0;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m require_at_least_one_of \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m {chan \u001b[38;5;28;01mfor\u001b[39;00m chan, _ \u001b[38;5;129;01min\u001b[39;00m filtered} \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(require_at_least_one_of):\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(\n\u001b[1;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust write to at least one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequire_at_least_one_of\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         )\n\u001b[1;32m    142\u001b[0m write: TYPE_SEND \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m][CONFIG_KEY_SEND]\n\u001b[1;32m    143\u001b[0m write(sends \u001b[38;5;241m+\u001b[39m filtered)\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Must write to at least one of ['user_q', 'chat_history', 'lst_data', 'output']"
     ]
    }
   ],
   "source": [
    "out = graph.invoke(input={\"user_q\":q, \"chat_history\":[]})\n",
    "out[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad9b3f-40f2-4a26-8282-d2662b7fe053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b721ed2-832a-4d48-9609-c76f55d9452e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e2eec-477f-4f77-8ecd-98a7a531fcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc0614-d706-4d7a-b929-c277a836b812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370aeccd-2ea8-46ff-ae84-c1aa347ed001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66624373-a123-45ea-a01f-90350eed25f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mBrowser:\u001b[0m 8 Google Employees Invented Modern AI. Here's the Inside ... The Transformer. Attention Is All You Need Figure 1. The goal is to understand why the Transformer was so groundbreaking and how it achieves these capabilities by implementing each part manually, relying only on NumPy without any out-of-the-box packages from Keras or PyTorch. Being a deep-learning beginner myself, I will try to add as much ... The transformer model, introduced with the groundbreaking paper 'Attention is All You Need', has revolutionized NLP by shifting the paradigm from sequential processing to parallel attention ... The transformer is an architecture that relies on the concept of attention, a technique used to provide weights to different parts of an input sequence so that a better understanding of its ... The \"Attention is All You Need\" paper ushered in an era where AI could focus, prioritize, and synthesize information in a manner more akin to human cognition, leading to advanced generative ...\n",
      "\n",
      "\u001b[1;31mInstagram:\u001b[0m 1,856 likes, 54 comments - nvidiaai on February 29, 2024: \" Don't miss the insights from all 8 authors of the groundbreaking paper \"Attention is All You Need\" that introduced transformers to the world. Hear from the experts on the technology that helped build every major #AI model. Join us at #GTC24 ️ See our link-in-bio for details, and to register.\". 4,876 likes, 53 comments - baxate_carter on March 30, 2024: \"Attention Is All You Need\". The Transformer architecture was first introduced in the 2017 paper \"Attention is All You Need\" by researchers at Google. Unlike previous sequence models such as RNNs, Transformer relies entirely on self-attention to model dependencies in sequential data like text. Here is a guide that will provide a step-by-step approach to decoding ... 21 likes, 4 comments - vidhvat on September 3, 2024: \"attention is all you need\". 1,622 likes, 11 comments - genetifics on May 14, 2024: \"El paper que lo cambio todo : 'Attention Is All You Need', fue publicado en el 2017 y escrito por científicos de Google. El estudio dio puerta abierta a incontables ideas innovadoras para nuevos modelos de inteligencia artificial. . #medicina #inteligenciaartificial #biologia #ciencia #bioingenieria #investigación #ingenieria\".\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "@tool(\"tool_browser\")\n",
    "def tool_browser(q: str) -> str:\n",
    "    \"\"\"Search on DuckDuckGo browser\"\"\"\n",
    "    return DuckDuckGoSearchRun().run(q)\n",
    "\n",
    "@tool(\"tool_instagram\")\n",
    "def tool_instagram(q: str) -> str:\n",
    "    '''Search on Instagram'''\n",
    "    return DuckDuckGoSearchRun().run(f\"site:instagram.com {q}\")\n",
    "\n",
    "print('\\x1b[1;31m'+'Browser:'+'\\x1b[0m', tool_browser(q) )\n",
    "print('\\n\\x1b[1;31m'+''+'Instagram:'+'\\x1b[0m', tool_instagram(q) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546f3c33-4873-45c1-97ff-167484805d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal Generative AI.\n",
      "The paper's title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Uszkoreit liked the sound of that word.\n",
      "An early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\n",
      "Some early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\n",
      "As of 2024, the paper has been cited more than 100,000 times.\n",
      "For their 100M-parameter Transformer model, they suggested learning rate should be linearly scaled up from 0 to maximal value for the first part of the training (i.e. 2% of the total number of training steps), and to use dropout, to stabilize training.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "@tool(\"tool_wikipedia\")\n",
    "def tool_wikipedia(q: str) -> str:\n",
    "    \"\"\"Search on Wikipedia\"\"\"\n",
    "    return wikipedia.summary(q)\n",
    "\n",
    "print( tool_wikipedia(q) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880822dc-a463-49c4-b215-1eb1f2ce5162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks in an encoder-decoder configuration. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer, based\n",
      "solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to be\n",
      "superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014\n",
      "English-to-German translation task, improving over the existing best results,\n",
      "including ensembles by over 2 BLEU. On the WMT 2014 English-to-French\n",
      "translation task, our model establishes a new single-model state-of-the-art\n",
      "BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\n",
      "of the training costs of the best models from the literature. We show that the\n",
      "Transformer generalizes well to other tasks by applying it successfully to\n",
      "English constituency parsing both with large and limited training data.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "@tool(\"tool_arxiv\")\n",
    "def tool_arxiv(arxiv_id: str) -> str:\n",
    "    \"\"\"Gets the abstract from an ArXiv paper given the ID. Useful for finding high-level context about a specific paper.\"\"\"\n",
    "    res = requests.get(f\"https://export.arxiv.org/abs/{arxiv_id}\")\n",
    "    get_abstract = re.compile(\n",
    "        r'<blockquote class=\"abstract mathjax\">\\s*<span class=\"descriptor\">Abstract:</span>\\s*(.*?)\\s*</blockquote>',\n",
    "        re.DOTALL)\n",
    "    res = get_abstract.search(res.text).group(1)\n",
    "    return res\n",
    "\n",
    "print( tool_arxiv(\"1706.03762\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac463c8-abd0-4061-806a-a277e2d161c7",
   "metadata": {},
   "source": [
    "### 3 - Single Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb7b4b-4510-4b64-90ea-783175adeca0",
   "metadata": {},
   "source": [
    "##### Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad911111-4058-4e2b-8136-1b202a0a58ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'yo', 'add_tool': '', 'add_source': ''}\n"
     ]
    }
   ],
   "source": [
    "@tool(\"final_answer\")\n",
    "def final_answer(answer: str, add_tool: str = \"\", add_source: str = \"\"):\n",
    "    \"\"\"Returns a natural language response to the user. There are 3 sections to be returned to the user:\n",
    "    - `answer`: the final natural language answer to the user's question, should provide as much context as possible.\n",
    "    - `add_tool`: additional information regarding which tool you used to get the answer.\n",
    "    - `add_source`: additional information regarding what's the source of the answer.\n",
    "    \"\"\"\n",
    "    return {\"answer\":answer, \"add_tool\":add_tool, \"add_source\":add_source}\n",
    "\n",
    "print( final_answer(\"yo\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf08ef-7a14-469e-adce-4bed3e46477e",
   "metadata": {},
   "source": [
    "##### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ed43c31-d77c-4f1d-b0b6-b21c7451e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are the oracle, the great AI decision maker. \n",
    "Given the user's query you must decide what to do with it based on the list of tools provided to you.\n",
    "\n",
    "Your goal is to provide the user with the best possible answer.\n",
    "Including key information about the source of information and the tools used.\n",
    "\n",
    "Note, when using a tool, you provide the tool name and the arguments to use in JSON format. \n",
    "For each call, you MUST ONLY use one tool AND the response format must ALWAYS be in the pattern:\n",
    "```json\n",
    "{\"name\":\"<tool_name>\", \"parameters\":{\"<tool_input_key>\":<tool_input_value>}}\n",
    "```\n",
    "Remember, do NOT use any tool with the same query more than once.\n",
    "Remember, if the user doesn't ask a specific question, you MUST use the `final_answer` tool directly.\n",
    "\n",
    "Every time the user asks a question, you take note of some keywords in the memory.\n",
    "Every time you find some information related to the user's question, you take note of some keywords in the memory.\n",
    "\n",
    "You should aim to collect information from a diverse range of sources before providing the answer to the user. \n",
    "Once you have collected plenty of information to answer the user's question use the `final_answer` tool.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f9067-1db5-4fc4-85ee-46f79fa94265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"assistant\", \"memory: {memory}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072692cb-d925-41c6-bc83-41c590d0ad4a",
   "metadata": {},
   "source": [
    "##### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3859255-a1ee-49a3-be8c-15b084373bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.agents import AgentAction\n",
    "import operator\n",
    "\n",
    "class AgentState(typing.TypedDict):\n",
    "    input: str #user's most recent query\n",
    "    chat_history: list[BaseMessage] #full chat\n",
    "    intermediate_steps: typing.Annotated[list[tuple[AgentAction, str]], operator.add] #record of all actions it does "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e8e49f5-7d3f-4c1b-bf58-e7376aa6f80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-09-10T15:09:53.824973Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 12,\n",
      " 'eval_duration': 1930824000,\n",
      " 'load_duration': 45695956,\n",
      " 'message': {'content': '{\"name\":\"final_answer\", \"parameters\":{}}',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'llama3.1',\n",
      " 'prompt_eval_count': 438,\n",
      " 'prompt_eval_duration': 29902821000,\n",
      " 'total_duration': 31898453594}\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "\n",
    "def get_tools(prompt: str, lst_tools: list[dict]):\n",
    "    str_tools = \"\\n\".join([str(tool) for tool in lst_tools])\n",
    "    return (f\"{prompt}.\\n\\n You may use the following tools:\\n{str_tools}\")\n",
    "\n",
    "res = ollama.chat(model=\"llama3.1\",\n",
    "                  messages=[{\"role\":\"system\", \"content\":get_tools(prompt=prompt,\n",
    "                                                        lst_tools=[tool_wikipedia, final_answer])},\n",
    "                            {\"role\":\"user\", \"content\":\"hi\"},\n",
    "                           ], format=\"json\")\n",
    "\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba1de0-169f-426b-96bc-a491dab9ff57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e1f1e-6388-4278-b27a-90ed9fd51caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04ef8332-ab87-4d05-8364-bfdd872c503a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool_name='final_answer', tool_input={}, tool_output=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class AgentAction(BaseModel):\n",
    "    tool_name: str\n",
    "    tool_input: dict\n",
    "    tool_output: str | None = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_ollama(cls, ollama_response: dict):\n",
    "        try:\n",
    "            output = json.loads(ollama_response[\"message\"][\"content\"])\n",
    "            return cls(tool_name=output[\"name\"], tool_input=output[\"parameters\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing ollama response:\\n{ollama_response}\\n\")\n",
    "            raise e\n",
    "\n",
    "    def __str__(self):\n",
    "        text = f\"Tool: {self.tool_name}\\nInput: {self.tool_input}\"\n",
    "        if self.tool_output is not None:\n",
    "            text += f\"\\nOutput: {self.tool_output}\"\n",
    "        return text\n",
    "\n",
    "\n",
    "action = AgentAction.from_ollama(res)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ad9fafd-f6ef-4528-84c5-839c164fcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_message(action: AgentAction):\n",
    "    assistant_content = json.dumps({\"name\": action.tool_name, \"parameters\": action.tool_input})\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "    user_message = {\"role\": \"user\", \"content\": action.tool_output}\n",
    "    return [assistant_message, user_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c268c7c-5dd0-429e-8a4d-1e6341b1f1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant',\n",
       "  'content': '{\"name\": \"xyz\", \"parameters\": {\"query\": \"something cool\"}}'},\n",
       " {'role': 'user', 'content': 'A fascinating tidbit of information'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_action = AgentAction(\n",
    "    tool_name=\"xyz\",\n",
    "    tool_input={\"query\": \"something cool\"},\n",
    "    tool_output=\"A fascinating tidbit of information\"\n",
    ")\n",
    "action_to_message(test_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d131fd-f945-4d1d-b409-a8350d683d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6da1261-ff61-4228-b68c-f1fa5a6a5f9b",
   "metadata": {},
   "source": [
    "### 4 - Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9aa95dc-900f-43e7-be75-0af38cd4e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scratchpad(intermediate_steps: list[AgentAction]):\n",
    "    # filter for actions that have a tool_output\n",
    "    intermediate_steps = [action for action in intermediate_steps if action.tool_output is not None]\n",
    "    # format the intermediate steps into a \"assistant\" input and \"user\" response list\n",
    "    scratch_pad_messages = []\n",
    "    for action in intermediate_steps:\n",
    "        scratch_pad_messages.extend(action_to_message(action))\n",
    "    return scratch_pad_messages\n",
    "\n",
    "def call_llm(user_input: str, chat_history: list[dict], intermediate_steps: list[AgentAction]) -> AgentAction:\n",
    "    # format the intermediate steps into a scratchpad\n",
    "    scratchpad = create_scratchpad(intermediate_steps)\n",
    "    # if the scratchpad is not empty, we add a small reminder message to the agent\n",
    "    if scratchpad:\n",
    "        scratchpad += [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Please continue, as a reminder my query was '{user_input}'. \"\n",
    "                \"Only answer to the original query, and nothing else — but use the \"\n",
    "                \"information I provided to you to do so. Provide as much \"\n",
    "                \"information as possible in the `answer` field of the \"\n",
    "                \"final_answer tool and remember to leave the contact details \"\n",
    "                \"of a promising looking restaurant.\"\n",
    "            )\n",
    "        }]\n",
    "        # we determine the list of tools available to the agent based on whether\n",
    "        # or not we have already used the search tool\n",
    "        tools_used = [action.tool_name for action in intermediate_steps]\n",
    "        tools = []\n",
    "        if \"search\" in tools_used:\n",
    "            # we do this because the LLM has a tendency to go off the rails\n",
    "            # and keep searching for the same thing\n",
    "            tools = [final_answer]\n",
    "            scratchpad[-1][\"content\"] = \" You must now use the final_answer tool.\"\n",
    "        else:\n",
    "            # this shouldn't happen, but we include it just in case\n",
    "            tools = [tool_wikipedia, final_answer]\n",
    "    else:\n",
    "        # this would indiciate we are on the first run, in which case we\n",
    "        # allow all tools to be used\n",
    "        tools = [tool_wikipedia, final_answer]\n",
    "    # construct our list of messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": get_tools(prompt=prompt, lst_tools=tools)},\n",
    "        *chat_history,\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "        *scratchpad,\n",
    "    ]\n",
    "    res = ollama.chat(\n",
    "        model=\"llama3.1\",\n",
    "        messages=messages,\n",
    "        format=\"json\",\n",
    "    )\n",
    "    return AgentAction.from_ollama(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "239fd597-8336-45e2-9c5a-50177aa85188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool_name='tool_wikipedia', tool_input={'query': 'Best pizzerias in Rome'}, tool_output=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's fake some chat history and test\n",
    "out = call_llm(\n",
    "    chat_history=[\n",
    "        {\"role\": \"user\", \"content\": \"hi there, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm good, thanks!\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'm currently in Rome\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"That's great, would you like any help?\"},\n",
    "    ],\n",
    "    user_input=\"yes, I'm looking for the best pizzeria near me\",\n",
    "    intermediate_steps=[]\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd6c54-b6da-46cc-9ee9-c1859b9b7350",
   "metadata": {},
   "source": [
    "### 5 - Graph Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b0f02-ca85-489d-9e43-3a91efafcd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8aeb2e6-51da-4fa8-b290-5281916a9a87",
   "metadata": {},
   "source": [
    "### 6 - Multiple Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94278194-0772-4ea6-9df9-0934486a08f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
