{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8214420c-1936-472c-930e-1f4c9efb3809",
   "metadata": {},
   "source": [
    "# GenAI with Python: Agents from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53219017-c57b-4ae8-8442-b89f95ff8bdc",
   "metadata": {},
   "source": [
    "###### [Article: TowardsDataScience]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd5a8e7-d0ca-4c6c-88b4-1c1e27591b84",
   "metadata": {},
   "source": [
    "### 0 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f53cd6-8254-40ff-aca4-ca18a1bc7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain --> 0.2.14\n",
    "#pip install langgraph --> 0.2.19\n",
    "#pip install ollama --> 0.3.1\n",
    "#pip install semantic-router --> 0.0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac37da15-9f5f-46b7-99cb-7bb4301ef088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.1',\n",
       " 'created_at': '2024-09-11T14:26:15.101194Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"I don't have any information about deaths that occurred on September 9, 2024. My knowledge cutoff is March 1, 2023, and I do not have real-time access to information. If you're looking for information on a specific person or event, I'd be happy to try to help with what I know up to my cutoff date.\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 12084964522,\n",
       " 'load_duration': 31783841,\n",
       " 'prompt_eval_count': 22,\n",
       " 'prompt_eval_duration': 164199000,\n",
       " 'eval_count': 74,\n",
       " 'eval_duration': 11883766000}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "llm = \"llama3.1\"\n",
    "q = '''who died on September 9, 2024?'''\n",
    "\n",
    "res = ollama.chat(model=llm, \n",
    "                  messages=[{\"role\":\"system\", \"content\":\"\"},\n",
    "                            {\"role\":\"user\", \"content\":q}])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c648ef-bed3-4179-8dcb-d6678abc0faa",
   "metadata": {},
   "source": [
    "### 1 - Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6aa2dbe-0aa0-4d57-964b-b8c7aac331e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94aab20-4864-49cf-a1df-f971e1ede1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search in the United States of people who died on September 9, 2024 obituaries and condolences. Find an obituary, get service details, leave condolence messages or send flowers or gifts in memory of a loved one. FILE - James Earl Jones arrives before the 84th Academy Awards on Sunday, Feb. 26, 2012, in the Hollywood section of Los Angeles. Jones, who overcame racial prejudice and a severe stutter to become a celebrated icon of stage and screen has died at age 93. His agent, Barry McPherson, confirmed Jones died Monday morning, Sept. 9, 2024, at home. James Earl Jones, revered actor who voiced Darth Vader in Star Wars, starred in Field of Dreams' died September 9 at his home in Dutchess County, NY. He was 93. ... September 9, 2024 1:33pm. Events. Deaths. Sep 1 John Schultz, Australian Football HOF ruckman (Brownlow Medal 1960; All Australian 1961; Victoria 24 games; Footscray FC), dies at 85. Sep 2 Aleksandr Medved, Ukrainian freestyle wrestler (Olympic gold USSR heavyweight 1964, 68, 72; World C'ship gold x 7), dies at 86. Sept. 11, 2024, 5:03 a.m. ET. ... \"Since September 11, 2001, ... More firefighters have now died from 9/11-related illnesses than in the attacks that day (343). The Fire Department also says ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "def browser(q: str) -> str:\n",
    "    \"\"\"Search on DuckDuckGo browser\"\"\"\n",
    "    return DuckDuckGoSearchRun().run(q)\n",
    "\n",
    "print( browser(q) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3950dd1b-f78d-4e12-b279-68f113242ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'browser',\n",
       "  'description': 'Search on DuckDuckGo browser',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'q': {'description': None, 'type': 'string'}},\n",
       "   'required': []}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_router.utils.function_call import FunctionSchema\n",
    "\n",
    "tool_browser = FunctionSchema(browser).to_ollama()\n",
    "tool_browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccb04de-9059-4d23-bc5c-8a84c02be92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'yo', 'add_tool': '', 'add_source': ''}\n"
     ]
    }
   ],
   "source": [
    "def final_answer(answer: str, add_tool: str = \"\", add_source: str = \"\"):\n",
    "    \"\"\"Returns a natural language response to the user. There are 3 sections to be returned to the user:\n",
    "    - `answer`: the final natural language answer to the user's question, should provide as much context as possible.\n",
    "    - `add_tool`: additional information regarding which tool you used to get the answer.\n",
    "    - `add_source`: additional information regarding what's the source of the answer.\n",
    "    \"\"\"\n",
    "    return {\"answer\":answer, \"add_tool\":add_tool, \"add_source\":add_source}\n",
    "\n",
    "print( final_answer(\"yo\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da5fd0c-fab4-4920-a078-d701eeac88c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'final_answer',\n",
       "  'description': \"Returns a natural language response to the user. There are 3 sections to be returned to the user:\\n- `answer`: the final natural language answer to the user's question, should provide as much context as possible.\\n- `add_tool`: additional information regarding which tool you used to get the answer.\\n- `add_source`: additional information regarding what's the source of the answer.\",\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'answer': {'description': None, 'type': 'string'},\n",
       "    'add_tool': {'description': None, 'type': 'string'},\n",
       "    'add_source': {'description': None, 'type': 'string'}},\n",
       "   'required': ['add_tool', 'add_source']}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_final_answer = FunctionSchema(final_answer).to_ollama()\n",
    "tool_final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182ae77-bed0-4262-b559-3f9c5a4de070",
   "metadata": {},
   "source": [
    "### 2 - Decision Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "952d7dad-b331-4e74-8f2d-6301d0d39f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are the oracle, the great AI decision maker. \n",
    "Given the user's query you must decide what to do based on the list of tools provided to you.\n",
    "\n",
    "Your goal is to provide the user with the best possible answer.\n",
    "Including key information about the source of information and the tools used.\n",
    "\n",
    "Note, when using a tool, you provide the tool name and the arguments to use in JSON format. \n",
    "For each call, you MUST ONLY use one tool AND the response format must ALWAYS be in the pattern:\n",
    "```json\n",
    "{\"name\":\"<tool_name>\", \"parameters\": {\"<tool_input_key>\":<tool_input_value>}}\n",
    "```\n",
    "Remember, do NOT use any tool with the same query more than once.\n",
    "Remember, if the user doesn't ask a specific question, you MUST use the `final_answer` tool directly.\n",
    "\n",
    "Every time the user asks a question, you take note of some keywords in the memory.\n",
    "Every time you find some information related to the user's question, you take note of some keywords in the memory.\n",
    "\n",
    "You should aim to collect information from a diverse range of sources before providing the answer to the user. \n",
    "Once you have collected plenty of information to answer the user's question use the `final_answer` tool.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a2a288c-fe0a-43bb-af08-cebd7d12ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-09-11T14:05:03.849928Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 25,\n",
      " 'eval_duration': 4335864000,\n",
      " 'load_duration': 12845284113,\n",
      " 'message': {'content': '{\"name\": \"browser\", \"parameters\": {\"q\": \"deaths on '\n",
      "                        'September 9, 2024\"}}',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'llama3.1',\n",
      " 'prompt_eval_count': 514,\n",
      " 'prompt_eval_duration': 55079864000,\n",
      " 'total_duration': 72362090029}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def get_tools(prompt: str, lst_tools: list[dict]):\n",
    "    str_tools = \"\\n\".join([str(tool) for tool in lst_tools])\n",
    "    return (f\"{prompt}.\\n\\n You may use the following tools:\\n{str_tools}\")\n",
    "\n",
    "res = ollama.chat(model=llm,\n",
    "                  messages=[{\"role\":\"system\", \"content\":get_tools(prompt=prompt,\n",
    "                                                        lst_tools=[tool_browser, tool_final_answer])},\n",
    "                            {\"role\":\"user\", \"content\":q},\n",
    "                           ], format=\"json\")\n",
    "\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43adda9d-b885-4440-8625-816b9334b417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"final_answer\", \"parameters\": {\"answer\":\"Hello! How can I assist you today?\", \"add_tool\":\"\",\"add_source\":\"\"}}\n"
     ]
    }
   ],
   "source": [
    "res = ollama.chat(model=llm,\n",
    "                  messages=[{\"role\":\"system\", \"content\":get_tools(prompt=prompt,\n",
    "                                                        lst_tools=[tool_browser, tool_final_answer])},\n",
    "                            {\"role\":\"user\", \"content\":\"hello\"},\n",
    "                           ], format=\"json\")\n",
    "\n",
    "print(res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0ff2b-28cf-43f2-beda-cd620c4f534a",
   "metadata": {},
   "source": [
    "### 3 - Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe47f093-1152-46a9-98c1-6bf60a725603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from\n",
      " {\"name\": \"browser\", \"parameters\": {\"q\": \"deaths on September 9, 2024\"}} \n",
      "to\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataStructure(tool_name='browser', tool_input={'q': 'deaths on September 9, 2024'}, tool_output=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel #this is the standard class\n",
    "import json\n",
    "\n",
    "# Taking for example the last LLM response, I want this structure:\n",
    "# {tool_name='final_answer', \n",
    "#  tool_input={'answer': \"Hello! It's nice to meet you.\", \n",
    "#               'add_tool': 'None', \n",
    "#               'add_source': 'User greeting'}, \n",
    "#  tool_output=None}\n",
    "\n",
    "class DataStructure(BaseModel):\n",
    "    tool_name: str  #must be a string\n",
    "    tool_input: dict #must be a dictionary\n",
    "    tool_output: str | None = None #can be a string or None\n",
    "    \n",
    "    ## function to create a new instance of the class with a specific input\n",
    "    @classmethod\n",
    "    def from_ollama(cls, res: dict): #return the class itself\n",
    "        try:\n",
    "            out = json.loads(res[\"message\"][\"content\"])\n",
    "            return cls(tool_name=out[\"name\"], tool_input=out[\"parameters\"])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error from ollama:\\n{res}\\n\")\n",
    "            raise e\n",
    "    \n",
    "    ## function to define how the class prints stuff\n",
    "    def __str__(self):\n",
    "        text = f'''Tool: {self.tool_name}\n",
    "                 \\nInput: {self.tool_input}'''\n",
    "        if self.tool_output is not None:\n",
    "            text += f\"\\nOutput: {self.tool_output}\"\n",
    "        return text\n",
    "\n",
    "# test\n",
    "data = DataStructure.from_ollama(res)\n",
    "print(\"from\\n\", res[\"message\"][\"content\"], \"\\nto\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d37c6-a49f-47e4-9e83-89e0a1e7b111",
   "metadata": {},
   "source": [
    "### 4 - Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05a313a5-eab2-4bfc-a96d-7fd0cd0cd870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Messages in Memory will have this structure:\n",
    "[{'role': 'assistant',\n",
    "  'content': '{\"name\": \"final_answer\", \"parameters\": {\"answer\": \"Hello! How can I assist you today?\", \"add_tool\": \"\", \"add_source\": \"\"}}'},\n",
    " {'role': 'user', 'content': None}]\n",
    "'''\n",
    "\n",
    "def create_memory(lst_data: list[DataStructure], user_q: str) -> list:\n",
    "    ## create\n",
    "    memory = []\n",
    "    for data in [data for data in lst_data if data.tool_output is not None]:\n",
    "        memory.extend([\n",
    "            ### assistant message\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps({\"name\":data.tool_name, \"parameters\":data.tool_input})},\n",
    "            ### user message\n",
    "            {\"role\":\"user\", \"content\":data.tool_output}\n",
    "        ])\n",
    "    \n",
    "    ## add a reminder of the ultimate goal\n",
    "    if memory:\n",
    "        memory += [{\"role\": \"user\", \"content\": (f'''\n",
    "                Please continue, as a reminder my query was `{user_q}`.\n",
    "                Only answer to the original query, and nothing else, but use the information I gave you. \n",
    "                Provide as much information as possible when you use the final_answer tool.\n",
    "                ''')}]\n",
    "    return memory\n",
    "\n",
    "create_memory(lst_data=[data], user_q=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8587b83a-c1aa-4b78-a23a-ba1db31571b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [{\"role\": \"user\", \"content\": \"hi there, how are you?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"I'm good, thanks!\"},\n",
    "                {\"role\": \"user\", \"content\": \"I have a question\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"tell me\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc2702-61d5-4ed5-b958-8d0c29645c18",
   "metadata": {},
   "source": [
    "### 5 - Single Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "623dcfac-fde3-41a2-bc74-9aa47d1d1b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataStructure(tool_name='browser', tool_input={'q': 'September 9 2024 deaths'}, tool_output=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def agent(user_q: str, chat_history: list[dict], lst_data: list[DataStructure]) -> DataStructure:\n",
    "    ## start memory\n",
    "    memory = create_memory(lst_data=lst_data, user_q=user_q)\n",
    "    \n",
    "    ## track used tools\n",
    "    if memory:\n",
    "        tools_used = [data.tool_name for data in lst_data]\n",
    "        lst_tools = []\n",
    "        if \"tool_browser\" in tools_used:\n",
    "            lst_tools = [tool_final_answer]\n",
    "            memory[-1][\"content\"] = \"You must now use the final_answer tool.\"\n",
    "        else:\n",
    "            lst_tools = [tool_browser, tool_final_answer]\n",
    "    else:\n",
    "        lst_tools = [tool_browser, tool_final_answer]\n",
    "        \n",
    "    ## messages\n",
    "    messages = [{\"role\":\"system\", \"content\":get_tools(prompt, lst_tools)},\n",
    "                *chat_history,\n",
    "                {\"role\":\"user\", \"content\":user_q},\n",
    "                *memory]\n",
    "    \n",
    "    ## output\n",
    "    res = ollama.chat(model=llm, messages=messages, format=\"json\")\n",
    "    return DataStructure.from_ollama(res)\n",
    "\n",
    "# test\n",
    "out = agent(user_q=q, chat_history=chat_history, lst_data=[])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c1be915e-e4d7-4acd-b578-871dbd079071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Search in the United States of people who died on September 9, 2024 obituaries and condolences. Find an obituary, get service details, leave condolence messages or send flowers or gifts in memory of a loved one. James Earl Jones, revered actor who voiced Darth Vader in Star Wars, starred in Field of Dreams' died September 9 at his home in Dutchess County, NY. He was 93. ... Hollywood & Media Deaths In ... Overall, state media reported 21 deaths and at least 299 people injured from the weekend. A man checks his damaged boat on September 8, 2024, after Typhoon Yagi hit Ha Long Bay, in Quang Ninh ... September 9, 2024 funerals. Sep 9, 2024 14 hrs ago; Facebook; Twitter; WhatsApp; SMS; ... one-time notices of deaths are published in The TimesDaily and placed on our Web site at no charge ... Events. Deaths. Sep 1 John Schultz, Australian Football HOF ruckman (Brownlow Medal 1960; All Australian 1961; Victoria 24 games; Footscray FC), dies at 85. Sep 2 Aleksandr Medved, Ukrainian freestyle wrestler (Olympic gold USSR heavyweight 1964, 68, 72; World C'ship gold x 7), dies at 86.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use a tool\n",
    "browser(**out.tool_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab77b7-8380-4b4c-9914-c34c0404798f",
   "metadata": {},
   "source": [
    "### 6 - Graph Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58113b-1961-47da-8b99-25871048995e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ec36529-68dd-4b83-a319-b04c40c918d2",
   "metadata": {},
   "source": [
    "### 7 - Multiple Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307acef-0b93-4779-acdc-d32f55c90a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7ff20-c631-4848-ab01-ab316b7fe555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b192b-7ae7-47ce-978c-7bdeee211ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc0614-d706-4d7a-b929-c277a836b812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370aeccd-2ea8-46ff-ae84-c1aa347ed001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66624373-a123-45ea-a01f-90350eed25f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mBrowser:\u001b[0m 8 Google Employees Invented Modern AI. Here's the Inside ... The Transformer. Attention Is All You Need Figure 1. The goal is to understand why the Transformer was so groundbreaking and how it achieves these capabilities by implementing each part manually, relying only on NumPy without any out-of-the-box packages from Keras or PyTorch. Being a deep-learning beginner myself, I will try to add as much ... The transformer model, introduced with the groundbreaking paper 'Attention is All You Need', has revolutionized NLP by shifting the paradigm from sequential processing to parallel attention ... The transformer is an architecture that relies on the concept of attention, a technique used to provide weights to different parts of an input sequence so that a better understanding of its ... The \"Attention is All You Need\" paper ushered in an era where AI could focus, prioritize, and synthesize information in a manner more akin to human cognition, leading to advanced generative ...\n",
      "\n",
      "\u001b[1;31mInstagram:\u001b[0m 1,856 likes, 54 comments - nvidiaai on February 29, 2024: \" Don't miss the insights from all 8 authors of the groundbreaking paper \"Attention is All You Need\" that introduced transformers to the world. Hear from the experts on the technology that helped build every major #AI model. Join us at #GTC24 ️ See our link-in-bio for details, and to register.\". 4,876 likes, 53 comments - baxate_carter on March 30, 2024: \"Attention Is All You Need\". The Transformer architecture was first introduced in the 2017 paper \"Attention is All You Need\" by researchers at Google. Unlike previous sequence models such as RNNs, Transformer relies entirely on self-attention to model dependencies in sequential data like text. Here is a guide that will provide a step-by-step approach to decoding ... 21 likes, 4 comments - vidhvat on September 3, 2024: \"attention is all you need\". 1,622 likes, 11 comments - genetifics on May 14, 2024: \"El paper que lo cambio todo : 'Attention Is All You Need', fue publicado en el 2017 y escrito por científicos de Google. El estudio dio puerta abierta a incontables ideas innovadoras para nuevos modelos de inteligencia artificial. . #medicina #inteligenciaartificial #biologia #ciencia #bioingenieria #investigación #ingenieria\".\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "@tool(\"tool_browser\")\n",
    "def tool_browser(q: str) -> str:\n",
    "    \"\"\"Search on DuckDuckGo browser\"\"\"\n",
    "    return DuckDuckGoSearchRun().run(q)\n",
    "\n",
    "@tool(\"tool_instagram\")\n",
    "def tool_instagram(q: str) -> str:\n",
    "    '''Search on Instagram'''\n",
    "    return DuckDuckGoSearchRun().run(f\"site:instagram.com {q}\")\n",
    "\n",
    "print('\\x1b[1;31m'+'Browser:'+'\\x1b[0m', tool_browser(q) )\n",
    "print('\\n\\x1b[1;31m'+''+'Instagram:'+'\\x1b[0m', tool_instagram(q) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546f3c33-4873-45c1-97ff-167484805d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal Generative AI.\n",
      "The paper's title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Uszkoreit liked the sound of that word.\n",
      "An early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\n",
      "Some early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\n",
      "As of 2024, the paper has been cited more than 100,000 times.\n",
      "For their 100M-parameter Transformer model, they suggested learning rate should be linearly scaled up from 0 to maximal value for the first part of the training (i.e. 2% of the total number of training steps), and to use dropout, to stabilize training.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "@tool(\"tool_wikipedia\")\n",
    "def tool_wikipedia(q: str) -> str:\n",
    "    \"\"\"Search on Wikipedia\"\"\"\n",
    "    return wikipedia.summary(q)\n",
    "\n",
    "print( tool_wikipedia(q) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880822dc-a463-49c4-b215-1eb1f2ce5162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks in an encoder-decoder configuration. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer, based\n",
      "solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to be\n",
      "superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014\n",
      "English-to-German translation task, improving over the existing best results,\n",
      "including ensembles by over 2 BLEU. On the WMT 2014 English-to-French\n",
      "translation task, our model establishes a new single-model state-of-the-art\n",
      "BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\n",
      "of the training costs of the best models from the literature. We show that the\n",
      "Transformer generalizes well to other tasks by applying it successfully to\n",
      "English constituency parsing both with large and limited training data.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "@tool(\"tool_arxiv\")\n",
    "def tool_arxiv(arxiv_id: str) -> str:\n",
    "    \"\"\"Gets the abstract from an ArXiv paper given the ID. Useful for finding high-level context about a specific paper.\"\"\"\n",
    "    res = requests.get(f\"https://export.arxiv.org/abs/{arxiv_id}\")\n",
    "    get_abstract = re.compile(\n",
    "        r'<blockquote class=\"abstract mathjax\">\\s*<span class=\"descriptor\">Abstract:</span>\\s*(.*?)\\s*</blockquote>',\n",
    "        re.DOTALL)\n",
    "    res = get_abstract.search(res.text).group(1)\n",
    "    return res\n",
    "\n",
    "print( tool_arxiv(\"1706.03762\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac463c8-abd0-4061-806a-a277e2d161c7",
   "metadata": {},
   "source": [
    "### 3 - Single Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb7b4b-4510-4b64-90ea-783175adeca0",
   "metadata": {},
   "source": [
    "##### Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad911111-4058-4e2b-8136-1b202a0a58ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'yo', 'add_tool': '', 'add_source': ''}\n"
     ]
    }
   ],
   "source": [
    "@tool(\"final_answer\")\n",
    "def final_answer(answer: str, add_tool: str = \"\", add_source: str = \"\"):\n",
    "    \"\"\"Returns a natural language response to the user. There are 3 sections to be returned to the user:\n",
    "    - `answer`: the final natural language answer to the user's question, should provide as much context as possible.\n",
    "    - `add_tool`: additional information regarding which tool you used to get the answer.\n",
    "    - `add_source`: additional information regarding what's the source of the answer.\n",
    "    \"\"\"\n",
    "    return {\"answer\":answer, \"add_tool\":add_tool, \"add_source\":add_source}\n",
    "\n",
    "print( final_answer(\"yo\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf08ef-7a14-469e-adce-4bed3e46477e",
   "metadata": {},
   "source": [
    "##### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ed43c31-d77c-4f1d-b0b6-b21c7451e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are the oracle, the great AI decision maker. \n",
    "Given the user's query you must decide what to do with it based on the list of tools provided to you.\n",
    "\n",
    "Your goal is to provide the user with the best possible answer.\n",
    "Including key information about the source of information and the tools used.\n",
    "\n",
    "Note, when using a tool, you provide the tool name and the arguments to use in JSON format. \n",
    "For each call, you MUST ONLY use one tool AND the response format must ALWAYS be in the pattern:\n",
    "```json\n",
    "{\"name\":\"<tool_name>\", \"parameters\":{\"<tool_input_key>\":<tool_input_value>}}\n",
    "```\n",
    "Remember, do NOT use any tool with the same query more than once.\n",
    "Remember, if the user doesn't ask a specific question, you MUST use the `final_answer` tool directly.\n",
    "\n",
    "Every time the user asks a question, you take note of some keywords in the memory.\n",
    "Every time you find some information related to the user's question, you take note of some keywords in the memory.\n",
    "\n",
    "You should aim to collect information from a diverse range of sources before providing the answer to the user. \n",
    "Once you have collected plenty of information to answer the user's question use the `final_answer` tool.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f9067-1db5-4fc4-85ee-46f79fa94265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"assistant\", \"memory: {memory}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072692cb-d925-41c6-bc83-41c590d0ad4a",
   "metadata": {},
   "source": [
    "##### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3859255-a1ee-49a3-be8c-15b084373bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.agents import AgentAction\n",
    "import operator\n",
    "\n",
    "class AgentState(typing.TypedDict):\n",
    "    input: str #user's most recent query\n",
    "    chat_history: list[BaseMessage] #full chat\n",
    "    intermediate_steps: typing.Annotated[list[tuple[AgentAction, str]], operator.add] #record of all actions it does "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e8e49f5-7d3f-4c1b-bf58-e7376aa6f80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-09-10T15:09:53.824973Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 12,\n",
      " 'eval_duration': 1930824000,\n",
      " 'load_duration': 45695956,\n",
      " 'message': {'content': '{\"name\":\"final_answer\", \"parameters\":{}}',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'llama3.1',\n",
      " 'prompt_eval_count': 438,\n",
      " 'prompt_eval_duration': 29902821000,\n",
      " 'total_duration': 31898453594}\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "\n",
    "def get_tools(prompt: str, lst_tools: list[dict]):\n",
    "    str_tools = \"\\n\".join([str(tool) for tool in lst_tools])\n",
    "    return (f\"{prompt}.\\n\\n You may use the following tools:\\n{str_tools}\")\n",
    "\n",
    "res = ollama.chat(model=\"llama3.1\",\n",
    "                  messages=[{\"role\":\"system\", \"content\":get_tools(prompt=prompt,\n",
    "                                                        lst_tools=[tool_wikipedia, final_answer])},\n",
    "                            {\"role\":\"user\", \"content\":\"hi\"},\n",
    "                           ], format=\"json\")\n",
    "\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba1de0-169f-426b-96bc-a491dab9ff57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e1f1e-6388-4278-b27a-90ed9fd51caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04ef8332-ab87-4d05-8364-bfdd872c503a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool_name='final_answer', tool_input={}, tool_output=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class AgentAction(BaseModel):\n",
    "    tool_name: str\n",
    "    tool_input: dict\n",
    "    tool_output: str | None = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_ollama(cls, ollama_response: dict):\n",
    "        try:\n",
    "            output = json.loads(ollama_response[\"message\"][\"content\"])\n",
    "            return cls(tool_name=output[\"name\"], tool_input=output[\"parameters\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing ollama response:\\n{ollama_response}\\n\")\n",
    "            raise e\n",
    "\n",
    "    def __str__(self):\n",
    "        text = f\"Tool: {self.tool_name}\\nInput: {self.tool_input}\"\n",
    "        if self.tool_output is not None:\n",
    "            text += f\"\\nOutput: {self.tool_output}\"\n",
    "        return text\n",
    "\n",
    "\n",
    "action = AgentAction.from_ollama(res)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ad9fafd-f6ef-4528-84c5-839c164fcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_message(action: AgentAction):\n",
    "    assistant_content = json.dumps({\"name\": action.tool_name, \"parameters\": action.tool_input})\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "    user_message = {\"role\": \"user\", \"content\": action.tool_output}\n",
    "    return [assistant_message, user_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c268c7c-5dd0-429e-8a4d-1e6341b1f1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant',\n",
       "  'content': '{\"name\": \"xyz\", \"parameters\": {\"query\": \"something cool\"}}'},\n",
       " {'role': 'user', 'content': 'A fascinating tidbit of information'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_action = AgentAction(\n",
    "    tool_name=\"xyz\",\n",
    "    tool_input={\"query\": \"something cool\"},\n",
    "    tool_output=\"A fascinating tidbit of information\"\n",
    ")\n",
    "action_to_message(test_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d131fd-f945-4d1d-b409-a8350d683d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6da1261-ff61-4228-b68c-f1fa5a6a5f9b",
   "metadata": {},
   "source": [
    "### 4 - Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9aa95dc-900f-43e7-be75-0af38cd4e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scratchpad(intermediate_steps: list[AgentAction]):\n",
    "    # filter for actions that have a tool_output\n",
    "    intermediate_steps = [action for action in intermediate_steps if action.tool_output is not None]\n",
    "    # format the intermediate steps into a \"assistant\" input and \"user\" response list\n",
    "    scratch_pad_messages = []\n",
    "    for action in intermediate_steps:\n",
    "        scratch_pad_messages.extend(action_to_message(action))\n",
    "    return scratch_pad_messages\n",
    "\n",
    "def call_llm(user_input: str, chat_history: list[dict], intermediate_steps: list[AgentAction]) -> AgentAction:\n",
    "    # format the intermediate steps into a scratchpad\n",
    "    scratchpad = create_scratchpad(intermediate_steps)\n",
    "    # if the scratchpad is not empty, we add a small reminder message to the agent\n",
    "    if scratchpad:\n",
    "        scratchpad += [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Please continue, as a reminder my query was '{user_input}'. \"\n",
    "                \"Only answer to the original query, and nothing else — but use the \"\n",
    "                \"information I provided to you to do so. Provide as much \"\n",
    "                \"information as possible in the `answer` field of the \"\n",
    "                \"final_answer tool and remember to leave the contact details \"\n",
    "                \"of a promising looking restaurant.\"\n",
    "            )\n",
    "        }]\n",
    "        # we determine the list of tools available to the agent based on whether\n",
    "        # or not we have already used the search tool\n",
    "        tools_used = [action.tool_name for action in intermediate_steps]\n",
    "        tools = []\n",
    "        if \"search\" in tools_used:\n",
    "            # we do this because the LLM has a tendency to go off the rails\n",
    "            # and keep searching for the same thing\n",
    "            tools = [final_answer]\n",
    "            scratchpad[-1][\"content\"] = \" You must now use the final_answer tool.\"\n",
    "        else:\n",
    "            # this shouldn't happen, but we include it just in case\n",
    "            tools = [tool_wikipedia, final_answer]\n",
    "    else:\n",
    "        # this would indiciate we are on the first run, in which case we\n",
    "        # allow all tools to be used\n",
    "        tools = [tool_wikipedia, final_answer]\n",
    "    # construct our list of messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": get_tools(prompt=prompt, lst_tools=tools)},\n",
    "        *chat_history,\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "        *scratchpad,\n",
    "    ]\n",
    "    res = ollama.chat(\n",
    "        model=\"llama3.1\",\n",
    "        messages=messages,\n",
    "        format=\"json\",\n",
    "    )\n",
    "    return AgentAction.from_ollama(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "239fd597-8336-45e2-9c5a-50177aa85188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool_name='tool_wikipedia', tool_input={'query': 'Best pizzerias in Rome'}, tool_output=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's fake some chat history and test\n",
    "out = call_llm(\n",
    "    chat_history=[\n",
    "        {\"role\": \"user\", \"content\": \"hi there, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm good, thanks!\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'm currently in Rome\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"That's great, would you like any help?\"},\n",
    "    ],\n",
    "    user_input=\"yes, I'm looking for the best pizzeria near me\",\n",
    "    intermediate_steps=[]\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd6c54-b6da-46cc-9ee9-c1859b9b7350",
   "metadata": {},
   "source": [
    "### 5 - Graph Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b0f02-ca85-489d-9e43-3a91efafcd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8aeb2e6-51da-4fa8-b290-5281916a9a87",
   "metadata": {},
   "source": [
    "### 6 - Multiple Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94278194-0772-4ea6-9df9-0934486a08f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
