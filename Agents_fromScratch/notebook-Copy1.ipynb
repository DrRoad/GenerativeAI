{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8214420c-1936-472c-930e-1f4c9efb3809",
   "metadata": {},
   "source": [
    "# GenAI with Python: Agents from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53219017-c57b-4ae8-8442-b89f95ff8bdc",
   "metadata": {},
   "source": [
    "###### [Article: TowardsDataScience]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd5a8e7-d0ca-4c6c-88b4-1c1e27591b84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 0 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f53cd6-8254-40ff-aca4-ca18a1bc7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain --> 0.2.14\n",
    "#pip install langgraph --> 0.2.19\n",
    "#pip install ollama --> 0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac37da15-9f5f-46b7-99cb-7bb4301ef088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.1',\n",
       " 'created_at': '2024-09-15T01:03:51.041113Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"I can't predict the future or provide information about specific individuals who may pass away. However, I can suggest some ways you could find out if someone notable died on that date. Would that help?\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 10346416766,\n",
       " 'load_duration': 42745451,\n",
       " 'prompt_eval_count': 22,\n",
       " 'prompt_eval_duration': 3660800000,\n",
       " 'eval_count': 41,\n",
       " 'eval_duration': 6637400000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "llm = \"llama3.1\"\n",
    "q = '''who died on September 9, 2024?'''\n",
    "\n",
    "res = ollama.chat(model=llm, \n",
    "                  messages=[{\"role\":\"system\", \"content\":\"\"},\n",
    "                            {\"role\":\"user\", \"content\":q}])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c648ef-bed3-4179-8dcb-d6678abc0faa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1 - Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6aa2dbe-0aa0-4d57-964b-b8c7aac331e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search in the United States of people who died on September 9, 2024 obituaries and condolences. Find an obituary, get service details, leave condolence messages or send flowers or gifts in memory of a loved one. James Earl Jones, revered actor who voiced Darth Vader in Star Wars, starred in Field of Dreams' died September 9 at his home in Dutchess County, NY. He was 93. ... September 9, 2024 1:33pm. James Earl Jones, legendary actor who was the voice of Darth Vader and Mufasa, died September 9, 2024 at his home in Dutchess County, New York at the age of 93. FILE - James Earl Jones arrives before the 84th Academy Awards on Sunday, Feb. 26, 2012, in the Hollywood section of Los Angeles. Jones, who overcame racial prejudice and a severe stutter to become a celebrated icon of stage and screen has died at age 93. His agent, Barry McPherson, confirmed Jones died Monday morning, Sept. 9, 2024, at home. Actor James Earl Jones, known for his booming voice and many memorable performances, died Monday morning, according to his agent Barry McPherson. ... Mon September 9, 2024 Link Copied!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "@tool(\"tool_browser\")\n",
    "def tool_browser(q:str) -> str:\n",
    "    \"\"\"Search on DuckDuckGo browser\"\"\"\n",
    "    return DuckDuckGoSearchRun().run(q)\n",
    "\n",
    "print( tool_browser(q) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "95293ae9-3422-4f48-af4c-6eb4be196416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n"
     ]
    }
   ],
   "source": [
    "@tool(\"final_answer\")\n",
    "def final_answer(text:str) -> str:\n",
    "    \"\"\"Returns a natural language response to the user. \n",
    "    You should provide as much context as possible and specify the source of the information.\n",
    "    \"\"\"\n",
    "    return text\n",
    "\n",
    "print( final_answer(\"yo\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "79657a90-ef3c-42db-a067-6829018f3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_tools = {\"tool_browser\":tool_browser, \n",
    "             \"final_answer\":final_answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182ae77-bed0-4262-b559-3f9c5a4de070",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2 - Decision Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "952d7dad-b331-4e74-8f2d-6301d0d39f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You know everything, you must answer every question from the user, you can use the list of tools provided to you.\n",
    "Your goal is to provide the user with the best possible answer, including key information about the sources and tools used.\n",
    "\n",
    "Note, when using a tool, you provide the tool name and the arguments to use in JSON format. \n",
    "For each call, you MUST ONLY use one tool AND the response format must ALWAYS be in the pattern:\n",
    "```json\n",
    "{\"name\":\"<tool_name>\", \"parameters\": {\"<tool_input_key>\":<tool_input_value>}}\n",
    "```\n",
    "Remember, do NOT use any tool with the same query more than once.\n",
    "Remember, if the user doesn't ask a specific question, you MUST use the `final_answer` tool directly.\n",
    "\n",
    "Every time the user asks a question, you take note of some keywords in the memory.\n",
    "Every time you find some information related to the user's question, you take note of some keywords in the memory.\n",
    "\n",
    "You should aim to collect information from a diverse range of sources before providing the answer to the user. \n",
    "Once you have collected plenty of information to answer the user's question use the `final_answer` tool.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "52813d8f-2579-4925-98df-55bfa5df1d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the following tools:\n",
      "1. `tool_browser`: Search on DuckDuckGo browser\n",
      "2. `final_answer`: Returns a natural language response to the user. \n",
      "    You should provide as much context as possible and specify the source of the information.\n"
     ]
    }
   ],
   "source": [
    "str_tools = \"\\n\".join([str(n+1)+\". `\"+str(v.name)+\"`: \"+str(v.description) for n,v in enumerate(dic_tools.values())])\n",
    "\n",
    "prompt_tools = f\"You can use the following tools:\\n{str_tools}\"\n",
    "print(prompt_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c76fbe59-b967-458f-b8f3-a99b54bf4a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-09-16T05:32:08.589481Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 24,\n",
      " 'eval_duration': 3961527000,\n",
      " 'load_duration': 9547200792,\n",
      " 'message': {'content': '{\"name\":\"final_answer\", \"parameters\": {\"text\":\"Hello! '\n",
      "                        'I\\'m happy to chat with you.\"}}',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'llama3.1',\n",
      " 'prompt_eval_count': 309,\n",
      " 'prompt_eval_duration': 30110343000,\n",
      " 'total_duration': 43664088905}\n"
     ]
    }
   ],
   "source": [
    "# LLM deciding what tool to use\n",
    "from pprint import pprint\n",
    "\n",
    "res = ollama.chat(model=llm,\n",
    "                  messages=[{\"role\":\"system\", \"content\":prompt+\"\\n\"+prompt_tools},\n",
    "                            {\"role\":\"user\", \"content\":\"hello\"}\n",
    "                           ], format=\"json\")\n",
    "\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9e61195b-cd68-4e22-b335-3a88477c8f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\":\"tool_browser\", \"parameters\": {\"q\":\"September 9 2024 deaths\"}}'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM deciding what tool to use (output format = json)\n",
    "res = ollama.chat(model=llm,\n",
    "                  messages=[{\"role\":\"system\", \"content\":prompt+\"\\n\"+prompt_tools},\n",
    "                            {\"role\":\"user\", \"content\":q}\n",
    "                           ], format=\"json\")\n",
    "\n",
    "res[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b97d4e7f-4d9a-439c-a508-9c40cc44a3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'September 9 2024 deaths'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input = json.loads(res[\"message\"][\"content\"])[\"parameters\"][\"q\"]\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b37eb027-6fe8-448c-8094-570679c89fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool output:\n",
      " Search in the United States of people who died on September 9, 2024 obituaries and condolences. Find an obituary, get service details, leave condolence messages or send flowers or gifts in memory of a loved one. Famous deaths in September 2024. Learn about 13 historical figures, notable people and celebrities who died in Sep 2024 like James Earl Jones and Alberto Fujimori. Menu. ... James Earl Jones (1931-2024) Sep 9 American Tony, Emmy, Grammy, and Golden Globe winning actor (The Great White Hope; Star Wars - \"voice of Darth Vader\"; Field Of Dreams\"), ... James Earl Jones, revered actor who voiced Darth Vader in Star Wars, starred in Field of Dreams' died September 9 at his home in Dutchess County, NY. He was 93. ... Hollywood & Media Deaths In ... 1926 William S. Scarborough, American linguist and author (Birds of Aristophanes), dies at 74. 1931 Lujo Brentano, German economist and social reformer, dies at 86. 1934 Roger Fry, English artist and art critic, dies at 67. 1941 Gustav Ehrismann, German author and expert on the German language, dies at 85. James Earl Jones sits for a portrait in September 2014. Jesse Dittmar for The Washington Post/Getty Images Jones, seen here in 1960, was born in Arkabutla, Mississippi, in 1931.\n",
      "\n",
      "llm output:\n",
      " According to the provided information, James Earl Jones died on September 9, 2024.\n"
     ]
    }
   ],
   "source": [
    "# LLM with context\n",
    "context = tool_browser(input)\n",
    "print(\"tool output:\\n\", context)\n",
    "\n",
    "output = ollama.chat(model=llm, \n",
    "                  messages=[{\"role\":\"system\", \"content\":\"Give the most accurate answer using the folling information:\\n\"+context},\n",
    "                            {\"role\":\"user\", \"content\":q}])\n",
    "\n",
    "print(\"\\nllm output:\\n\", output[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0ff2b-28cf-43f2-beda-cd620c4f534a",
   "metadata": {},
   "source": [
    "### 3 - Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2a13d-72eb-4fcb-a925-2326aed74869",
   "metadata": {},
   "source": [
    "##### Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "fe47f093-1152-46a9-98c1-6bf60a725603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from\n",
      " {\"name\":\"tool_browser\", \"parameters\": {\"q\":\"September 9 2024 deaths\"}} \n",
      "to\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataStructure(tool_name='tool_browser', tool_input={'q': 'September 9 2024 deaths'}, tool_output=None)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel #this is the standard class\n",
    "\n",
    "# Taking for example the last LLM response, I want this structure:\n",
    "# {tool_name: 'tool_browser', \n",
    "#  tool_input: {'q':'September 9 2024 deaths'}, \n",
    "#  tool_output: str( tool_browser({'q':'September 9 2024 deaths'})) }\n",
    "\n",
    "class DataStructure(BaseModel):\n",
    "    tool_name: str  #<--must be a string\n",
    "    tool_input: dict #<--must be a dictionary\n",
    "    tool_output: str | None = None #can be a string or None, default = None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_ollama(cls, res:dict): #<--return the class itself\n",
    "        try:\n",
    "            out = json.loads(res[\"message\"][\"content\"])\n",
    "            return cls(tool_name=out[\"name\"], tool_input=out[\"parameters\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Ollama:\\n{res}\\n\")\n",
    "            raise e\n",
    "\n",
    "# test\n",
    "data = DataStructure.from_ollama(res)\n",
    "print(\"from\\n\", res[\"message\"][\"content\"], \"\\nto\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "cb5339f4-4a1e-454b-b764-a2e0346e3ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataStructure(tool_name='tool_browser', tool_input={'q': 'September 9 2024 deaths'}, tool_output='Search in the United States of people who died on September 9, 2024 obituaries and condolences. Find an obituary, get service details, leave condolence messages or send flowers or gifts in memory of a loved one. Famous deaths in September 2024. Learn about 13 historical figures, notable people and celebrities who died in Sep 2024 like James Earl Jones and Alberto Fujimori. Menu. ... James Earl Jones (1931-2024) Sep 9 American Tony, Emmy, Grammy, and Golden Globe winning actor (The Great White Hope; Star Wars - \"voice of Darth Vader\"; Field Of Dreams\"), ... Overall, state media reported 21 deaths and at least 299 people injured from the weekend. A man checks his damaged boat on September 8, 2024, after Typhoon Yagi hit Ha Long Bay, in Quang Ninh ... 1926 William S. Scarborough, American linguist and author (Birds of Aristophanes), dies at 74. 1931 Lujo Brentano, German economist and social reformer, dies at 86. 1934 Roger Fry, English artist and art critic, dies at 67. 1941 Gustav Ehrismann, German author and expert on the German language, dies at 85. The following notable deaths occurred in 2024. Names are reported under the date of death, in alphabetical order. A typical entry reports information in the following sequence: ... This page was last edited on 16 September 2024, at 07:00 (UTC). Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may ...')"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the tool output\n",
    "DataStructure(tool_name=\"tool_browser\", \n",
    "              tool_input={'q':'September 9 2024 deaths'}, \n",
    "              tool_output=str( tool_browser({'q':'September 9 2024 deaths'})) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d37c6-a49f-47e4-9e83-89e0a1e7b111",
   "metadata": {},
   "source": [
    "##### Memory System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "05a313a5-eab2-4bfc-a96d-7fd0cd0cd870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Messages in Memory will have this structure:\n",
    "[{'role':'assistant', 'content':'{\"name\":\"final_answer\", \"parameters\":{\"text\":\"How can I assist you today?\"}}'},\n",
    " {'role':'user', 'content':None}]\n",
    "'''\n",
    "\n",
    "def save_memory(lst_data:list[DataStructure], user_q:str) -> list:\n",
    "    ## create\n",
    "    memory = []\n",
    "    for data in [data for data in lst_data if data.tool_output is not None]:\n",
    "        memory.extend([\n",
    "            ### assistant message\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps({\"name\":data.tool_name, \"parameters\":data.tool_input})},\n",
    "            ### user message\n",
    "            {\"role\":\"user\", \"content\":data.tool_output}\n",
    "        ])\n",
    "    \n",
    "    ## add a reminder of the original goal\n",
    "    if memory:\n",
    "        memory += [{\"role\":\"user\", \"content\":(f'''\n",
    "                This is just a reminder that my original query was `{user_q}`.\n",
    "                Only answer to the original query, and nothing else, but use the information I gave you. \n",
    "                Provide as much information as possible when you use the `final_answer` tool.\n",
    "                ''')}]\n",
    "    return memory\n",
    "\n",
    "create_memory(lst_data=[data], user_q=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8587b83a-c1aa-4b78-a23a-ba1db31571b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [{\"role\": \"user\", \"content\": \"hi there, how are you?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"I'm good, thanks!\"},\n",
    "                {\"role\": \"user\", \"content\": \"I have a question\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"tell me\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc2702-61d5-4ed5-b958-8d0c29645c18",
   "metadata": {},
   "source": [
    "##### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "623dcfac-fde3-41a2-bc74-9aa47d1d1b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': '\\n'\n",
      "             'You know everything, you must answer every question from the '\n",
      "             'user, you can use the list of tools provided to you.\\n'\n",
      "             'Your goal is to provide the user with the best possible answer, '\n",
      "             'including key information about the sources and tools used.\\n'\n",
      "             '\\n'\n",
      "             'Note, when using a tool, you provide the tool name and the '\n",
      "             'arguments to use in JSON format. \\n'\n",
      "             'For each call, you MUST ONLY use one tool AND the response '\n",
      "             'format must ALWAYS be in the pattern:\\n'\n",
      "             '```json\\n'\n",
      "             '{\"name\":\"<tool_name>\", \"parameters\": '\n",
      "             '{\"<tool_input_key>\":<tool_input_value>}}\\n'\n",
      "             '```\\n'\n",
      "             'Remember, do NOT use any tool with the same query more than '\n",
      "             'once.\\n'\n",
      "             \"Remember, if the user doesn't ask a specific question, you MUST \"\n",
      "             'use the `final_answer` tool directly.\\n'\n",
      "             '\\n'\n",
      "             'Every time the user asks a question, you take note of some '\n",
      "             'keywords in the memory.\\n'\n",
      "             \"Every time you find some information related to the user's \"\n",
      "             'question, you take note of some keywords in the memory.\\n'\n",
      "             '\\n'\n",
      "             'You should aim to collect information from a diverse range of '\n",
      "             'sources before providing the answer to the user. \\n'\n",
      "             'Once you have collected plenty of information to answer the '\n",
      "             \"user's question use the `final_answer` tool.\\n\"\n",
      "             '\\n'\n",
      "             'You can use the following tools:\\n'\n",
      "             '1. `tool_browser`: Search on DuckDuckGo browser\\n'\n",
      "             '2. `final_answer`: Returns a natural language response to the '\n",
      "             'user. \\n'\n",
      "             '    You should provide as much context as possible and specify '\n",
      "             'the source of the information.',\n",
      "  'role': 'system'},\n",
      " {'content': 'hi there, how are you?', 'role': 'user'},\n",
      " {'content': \"I'm good, thanks!\", 'role': 'assistant'},\n",
      " {'content': 'I have a question', 'role': 'user'},\n",
      " {'content': 'tell me', 'role': 'assistant'},\n",
      " {'content': 'who died on September 9, 2024?', 'role': 'user'}]\n",
      "\n",
      "Agent: tool_name='tool_browser' tool_input={'q': 'September 9 2024 deaths'} tool_output=None\n"
     ]
    }
   ],
   "source": [
    "def create_agent(user_q:str, chat_history:list[dict], lst_data:list[DataStructure], lst_tools:list) -> DataStructure:\n",
    "    ## start memory\n",
    "    memory = save_memory(lst_data=lst_data, user_q=user_q)\n",
    "    \n",
    "    ## track used tools\n",
    "    if memory:\n",
    "        tools_used = [data.tool_name for data in lst_data]\n",
    "        if len(tools_used) >= len(lst_tools):\n",
    "            memory[-1][\"content\"] = \"You must now use the `final_answer` tool.\"\n",
    "        \n",
    "    ## messages\n",
    "    messages = [{\"role\":\"system\", \"content\":prompt+\"\\n\"+prompt_tools},\n",
    "                *chat_history,\n",
    "                {\"role\":\"user\", \"content\":user_q},\n",
    "                *memory]\n",
    "    pprint(messages)\n",
    "    \n",
    "    ## output\n",
    "    res = ollama.chat(model=llm, messages=messages, format=\"json\")\n",
    "    return DataStructure.from_ollama(res)\n",
    "\n",
    "# test\n",
    "agent = create_agent(user_q=q, chat_history=chat_history, lst_data=[], lst_tools=dic_tools.keys())\n",
    "print(\"\\nAgent:\", agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab77b7-8380-4b4c-9914-c34c0404798f",
   "metadata": {},
   "source": [
    "### 4 - Graph Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9aad9-b664-4cc1-8085-26364bf041a1",
   "metadata": {},
   "source": [
    "##### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "fd64eaa3-6bc7-41e0-86a3-6061ba24b468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'who died on September 9, 2024?',\n",
       " 'chat_history': [{'role': 'user', 'content': 'hi there, how are you?'},\n",
       "  {'role': 'assistant', 'content': \"I'm good, thanks!\"},\n",
       "  {'role': 'user', 'content': 'I have a question'},\n",
       "  {'role': 'assistant', 'content': 'tell me'}],\n",
       " 'lst_data': [DataStructure(tool_name='tool_browser', tool_input={'q': 'September 9 2024 deaths'}, tool_output=None)],\n",
       " 'output': {}}"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing\n",
    "\n",
    "class State(typing.TypedDict):\n",
    "    input: str\n",
    "    chat_history: list\n",
    "    lst_data: list\n",
    "    output: dict\n",
    "\n",
    "# test\n",
    "state = State({\"input\":q, \"chat_history\":chat_history, \"lst_data\":[data], \"output\":{}})\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0581b8-ca39-4e08-b279-f69b9fdedb2c",
   "metadata": {},
   "source": [
    "##### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "982a35b4-2523-44b7-99e8-8537a19ce0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- node_agent ---\n",
      "[{'content': '\\n'\n",
      "             'You know everything, you must answer every question from the '\n",
      "             'user, you can use the list of tools provided to you.\\n'\n",
      "             'Your goal is to provide the user with the best possible answer, '\n",
      "             'including key information about the sources and tools used.\\n'\n",
      "             '\\n'\n",
      "             'Note, when using a tool, you provide the tool name and the '\n",
      "             'arguments to use in JSON format. \\n'\n",
      "             'For each call, you MUST ONLY use one tool AND the response '\n",
      "             'format must ALWAYS be in the pattern:\\n'\n",
      "             '```json\\n'\n",
      "             '{\"name\":\"<tool_name>\", \"parameters\": '\n",
      "             '{\"<tool_input_key>\":<tool_input_value>}}\\n'\n",
      "             '```\\n'\n",
      "             'Remember, do NOT use any tool with the same query more than '\n",
      "             'once.\\n'\n",
      "             \"Remember, if the user doesn't ask a specific question, you MUST \"\n",
      "             'use the `final_answer` tool directly.\\n'\n",
      "             '\\n'\n",
      "             'Every time the user asks a question, you take note of some '\n",
      "             'keywords in the memory.\\n'\n",
      "             \"Every time you find some information related to the user's \"\n",
      "             'question, you take note of some keywords in the memory.\\n'\n",
      "             '\\n'\n",
      "             'You should aim to collect information from a diverse range of '\n",
      "             'sources before providing the answer to the user. \\n'\n",
      "             'Once you have collected plenty of information to answer the '\n",
      "             \"user's question use the `final_answer` tool.\\n\"\n",
      "             '\\n'\n",
      "             'You can use the following tools:\\n'\n",
      "             '1. `tool_browser`: Search on DuckDuckGo browser\\n'\n",
      "             '2. `final_answer`: Returns a natural language response to the '\n",
      "             'user. \\n'\n",
      "             '    You should provide as much context as possible and specify '\n",
      "             'the source of the information.',\n",
      "  'role': 'system'},\n",
      " {'content': 'hi there, how are you?', 'role': 'user'},\n",
      " {'content': \"I'm good, thanks!\", 'role': 'assistant'},\n",
      " {'content': 'I have a question', 'role': 'user'},\n",
      " {'content': 'tell me', 'role': 'assistant'},\n",
      " {'content': 'who died on September 9, 2024?', 'role': 'user'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Agent': [DataStructure(tool_name='tool_browser', tool_input={'q': 'September 9 2024 death'}, tool_output=None)]}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agent\n",
    "def node_agent(state):\n",
    "    print(\"--- node_agent ---\")\n",
    "    return {\"Agent\":[\n",
    "        create_agent(user_q=state[\"input\"], \n",
    "                     chat_history=state[\"chat_history\"], \n",
    "                     lst_data=state[\"lst_data\"], \n",
    "                     lst_tools=dic_tools.keys())\n",
    "            ]}\n",
    "\n",
    "# test\n",
    "node_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "7d66fa3e-7de0-4ea7-90e7-26665cb9b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- node_tool ---\n",
      "run_tool --> tool_browser(input={'q': 'September 9 2024 deaths'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lst_data': [DataStructure(tool_name='tool_browser', tool_input={'q': 'September 9 2024 deaths'}, tool_output='Search in the United States of people who died on September 9, 2024 obituaries and condolences. Find an obituary, get service details, leave condolence messages or send flowers or gifts in memory of a loved one. Famous deaths in September 2024. Learn about 13 historical figures, notable people and celebrities who died in Sep 2024 like James Earl Jones and Alberto Fujimori. Menu. ... James Earl Jones (1931-2024) Sep 9 American Tony, Emmy, Grammy, and Golden Globe winning actor (The Great White Hope; Star Wars - \"voice of Darth Vader\"; Field Of Dreams\"), ... Overall, state media reported 21 deaths and at least 299 people injured from the weekend. A man checks his damaged boat on September 8, 2024, after Typhoon Yagi hit Ha Long Bay, in Quang Ninh ... 1926 William S. Scarborough, American linguist and author (Birds of Aristophanes), dies at 74. 1931 Lujo Brentano, German economist and social reformer, dies at 86. 1934 Roger Fry, English artist and art critic, dies at 67. 1941 Gustav Ehrismann, German author and expert on the German language, dies at 85. The following notable deaths occurred in 2024. Names are reported under the date of death, in alphabetical order. A typical entry reports information in the following sequence: ... This page was last edited on 16 September 2024, at 07:00 (UTC). Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may ...')]}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tools\n",
    "def node_tool(state):\n",
    "    print(\"--- node_tool ---\")\n",
    "    data = state[\"lst_data\"][-1]\n",
    "    print(f\"run_tool --> {data.tool_name}(input={data.tool_input})\")\n",
    "    \n",
    "    output = DataStructure(tool_name=data.tool_name, \n",
    "                           tool_input=data.tool_input, \n",
    "                           tool_output=str(dic_tools[data.tool_name](data.tool_input)) )\n",
    "    \n",
    "    return {\"output\":output} if data.tool_name == \"final_answer\" else {\"lst_data\":[output]}\n",
    "\n",
    "# test\n",
    "node_tool(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b55681-db5b-4650-955a-875d1c7da163",
   "metadata": {},
   "source": [
    "##### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "fe58b028-e548-430c-a009-608b144aae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- edges ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tool_browser'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def edges(state):\n",
    "    print(\"--- edges ---\")\n",
    "    return state[\"lst_data\"][-1].tool_name if isinstance(state[\"lst_data\"], list) else \"final_answer\"\n",
    "\n",
    "edges(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb86f8a-f7c6-41d5-b314-c07c7b385ba9",
   "metadata": {},
   "source": [
    "##### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3559fc30-33ce-466a-88be-ebe85985084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "## start the graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "## add Agent node\n",
    "workflow.add_node(node=\"Agent\", action=node_agent) \n",
    "workflow.set_entry_point(key=\"Agent\")  #<--user query\n",
    "\n",
    "## add Tools nodes\n",
    "for k in dic_tools.keys():\n",
    "    workflow.add_node(node=k, action=node_tool)\n",
    "\n",
    "## edges from Agent\n",
    "workflow.add_conditional_edges(source=\"Agent\", path=edges)\n",
    "\n",
    "## edges to Agent\n",
    "for k in dic_tools.keys():\n",
    "    if k != \"final_answer\":\n",
    "        workflow.add_edge(start_key=k, end_key=\"Agent\")\n",
    "\n",
    "## end the graph\n",
    "workflow.add_edge(start_key=\"final_answer\", end_key=END)\n",
    "g = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5ccd3876-579f-463c-8a8f-29baea3c5574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(Image(\n",
    "    g.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf810a-4da2-46a3-91b5-87693c58c658",
   "metadata": {},
   "source": [
    "##### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "b38fc540-3b6f-44bf-aa08-652ae56ddeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- node_agent ---\n",
      "[{'content': '\\n'\n",
      "             'You know everything, you must answer every question from the '\n",
      "             'user, you can use the list of tools provided to you.\\n'\n",
      "             'Your goal is to provide the user with the best possible answer, '\n",
      "             'including key information about the sources and tools used.\\n'\n",
      "             '\\n'\n",
      "             'Note, when using a tool, you provide the tool name and the '\n",
      "             'arguments to use in JSON format. \\n'\n",
      "             'For each call, you MUST ONLY use one tool AND the response '\n",
      "             'format must ALWAYS be in the pattern:\\n'\n",
      "             '```json\\n'\n",
      "             '{\"name\":\"<tool_name>\", \"parameters\": '\n",
      "             '{\"<tool_input_key>\":<tool_input_value>}}\\n'\n",
      "             '```\\n'\n",
      "             'Remember, do NOT use any tool with the same query more than '\n",
      "             'once.\\n'\n",
      "             \"Remember, if the user doesn't ask a specific question, you MUST \"\n",
      "             'use the `final_answer` tool directly.\\n'\n",
      "             '\\n'\n",
      "             'Every time the user asks a question, you take note of some '\n",
      "             'keywords in the memory.\\n'\n",
      "             \"Every time you find some information related to the user's \"\n",
      "             'question, you take note of some keywords in the memory.\\n'\n",
      "             '\\n'\n",
      "             'You should aim to collect information from a diverse range of '\n",
      "             'sources before providing the answer to the user. \\n'\n",
      "             'Once you have collected plenty of information to answer the '\n",
      "             \"user's question use the `final_answer` tool.\\n\"\n",
      "             '\\n'\n",
      "             'You can use the following tools:\\n'\n",
      "             '1. `tool_browser`: Search on DuckDuckGo browser\\n'\n",
      "             '2. `final_answer`: Returns a natural language response to the '\n",
      "             'user. \\n'\n",
      "             '    You should provide as much context as possible and specify '\n",
      "             'the source of the information.',\n",
      "  'role': 'system'},\n",
      " {'content': 'hi there, how are you?', 'role': 'user'},\n",
      " {'content': \"I'm good, thanks!\", 'role': 'assistant'},\n",
      " {'content': 'I have a question', 'role': 'user'},\n",
      " {'content': 'tell me', 'role': 'assistant'},\n",
      " {'content': 'who died on September 9, 2024?', 'role': 'user'}]\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Must write to at least one of ['input', 'chat_history', 'lst_data', 'output']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[280], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m:q,\n\u001b[1;32m      2\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m:chat_history, \n\u001b[1;32m      3\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlst_data\u001b[39m\u001b[38;5;124m'\u001b[39m:[], \n\u001b[1;32m      4\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m:{}\n\u001b[1;32m      5\u001b[0m                        })\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1468\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1467\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1470\u001b[0m     config,\n\u001b[1;32m   1471\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1472\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1473\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1474\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1475\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1477\u001b[0m ):\n\u001b[1;32m   1478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1479\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1221\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1216\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1217\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1218\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1219\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1220\u001b[0m ):\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1222\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1223\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1224\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1225\u001b[0m     ):\n\u001b[1;32m   1226\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1228\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/runner.py:87\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, timeout, retry_policy)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m _panic_or_proceed(all_futures)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/runner.py:190\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls)\u001b[0m\n\u001b[1;32m    188\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/executor.py:59\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m         task\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/retry.py:26\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     24\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/utils/runnable.py:343\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/utils/runnable.py:121\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    120\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m--> 121\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    123\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/write.py:86\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m write\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     85\u001b[0m     ]\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_write(\n\u001b[1;32m     87\u001b[0m         config,\n\u001b[1;32m     88\u001b[0m         writes,\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_at_least_one_of \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/write.py:139\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[0;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m require_at_least_one_of \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m {chan \u001b[38;5;28;01mfor\u001b[39;00m chan, _ \u001b[38;5;129;01min\u001b[39;00m filtered} \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(require_at_least_one_of):\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(\n\u001b[1;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust write to at least one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequire_at_least_one_of\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         )\n\u001b[1;32m    142\u001b[0m write: TYPE_SEND \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m][CONFIG_KEY_SEND]\n\u001b[1;32m    143\u001b[0m write(sends \u001b[38;5;241m+\u001b[39m filtered)\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Must write to at least one of ['input', 'chat_history', 'lst_data', 'output']"
     ]
    }
   ],
   "source": [
    "out = g.invoke(input={'input':q,\n",
    "                      'chat_history':chat_history, \n",
    "                      'lst_data':[], \n",
    "                      'output':{}\n",
    "                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "8dd7ff20-c631-4848-ab01-ab316b7fe555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- node_agent ---\n",
      "[{'content': '\\n'\n",
      "             'You know everything, you must answer every question from the '\n",
      "             'user, you can use the list of tools provided to you.\\n'\n",
      "             'Your goal is to provide the user with the best possible answer, '\n",
      "             'including key information about the sources and tools used.\\n'\n",
      "             '\\n'\n",
      "             'Note, when using a tool, you provide the tool name and the '\n",
      "             'arguments to use in JSON format. \\n'\n",
      "             'For each call, you MUST ONLY use one tool AND the response '\n",
      "             'format must ALWAYS be in the pattern:\\n'\n",
      "             '```json\\n'\n",
      "             '{\"name\":\"<tool_name>\", \"parameters\": '\n",
      "             '{\"<tool_input_key>\":<tool_input_value>}}\\n'\n",
      "             '```\\n'\n",
      "             'Remember, do NOT use any tool with the same query more than '\n",
      "             'once.\\n'\n",
      "             \"Remember, if the user doesn't ask a specific question, you MUST \"\n",
      "             'use the `final_answer` tool directly.\\n'\n",
      "             '\\n'\n",
      "             'Every time the user asks a question, you take note of some '\n",
      "             'keywords in the memory.\\n'\n",
      "             \"Every time you find some information related to the user's \"\n",
      "             'question, you take note of some keywords in the memory.\\n'\n",
      "             '\\n'\n",
      "             'You should aim to collect information from a diverse range of '\n",
      "             'sources before providing the answer to the user. \\n'\n",
      "             'Once you have collected plenty of information to answer the '\n",
      "             \"user's question use the `final_answer` tool.\\n\"\n",
      "             '\\n'\n",
      "             'You can use the following tools:\\n'\n",
      "             '1. `tool_browser`: Search on DuckDuckGo browser\\n'\n",
      "             '2. `final_answer`: Returns a natural language response to the '\n",
      "             'user. \\n'\n",
      "             '    You should provide as much context as possible and specify '\n",
      "             'the source of the information.',\n",
      "  'role': 'system'},\n",
      " {'content': 'hi there, how are you?', 'role': 'user'},\n",
      " {'content': \"I'm good, thanks!\", 'role': 'assistant'},\n",
      " {'content': 'I have a question', 'role': 'user'},\n",
      " {'content': 'tell me', 'role': 'assistant'},\n",
      " {'content': 'who died on September 9, 2024?', 'role': 'user'}]\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Must write to at least one of ['input', 'chat_history', 'lst_data', 'output']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[279], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Give the first input state\u001b[39;00m\n\u001b[1;32m      2\u001b[0m steps \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m:q,\n\u001b[1;32m      3\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m'\u001b[39m:chat_history, \n\u001b[1;32m      4\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlst_data\u001b[39m\u001b[38;5;124m'\u001b[39m:[], \n\u001b[1;32m      5\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m:{}\n\u001b[1;32m      6\u001b[0m                        }) \n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(step)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1221\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1216\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1217\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1218\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1219\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1220\u001b[0m ):\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1222\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1223\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1224\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1225\u001b[0m     ):\n\u001b[1;32m   1226\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1228\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/runner.py:87\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, timeout, retry_policy)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m _panic_or_proceed(all_futures)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/runner.py:190\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls)\u001b[0m\n\u001b[1;32m    188\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/executor.py:59\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m         task\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/retry.py:26\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     24\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/utils/runnable.py:343\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/utils/runnable.py:121\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    120\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m--> 121\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    123\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/write.py:86\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m write\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     85\u001b[0m     ]\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_write(\n\u001b[1;32m     87\u001b[0m         config,\n\u001b[1;32m     88\u001b[0m         writes,\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_at_least_one_of \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GENAI2/lib/python3.11/site-packages/langgraph/pregel/write.py:139\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[0;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m require_at_least_one_of \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m {chan \u001b[38;5;28;01mfor\u001b[39;00m chan, _ \u001b[38;5;129;01min\u001b[39;00m filtered} \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(require_at_least_one_of):\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(\n\u001b[1;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust write to at least one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequire_at_least_one_of\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         )\n\u001b[1;32m    142\u001b[0m write: TYPE_SEND \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m][CONFIG_KEY_SEND]\n\u001b[1;32m    143\u001b[0m write(sends \u001b[38;5;241m+\u001b[39m filtered)\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Must write to at least one of ['input', 'chat_history', 'lst_data', 'output']"
     ]
    }
   ],
   "source": [
    "## Give the first input state\n",
    "steps = g.stream(input={'input':q,\n",
    "                        'chat_history':chat_history, \n",
    "                        'lst_data':[], \n",
    "                        'output':{}\n",
    "                       }) \n",
    "for step in steps:\n",
    "    print(step)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87447b1c-6c7a-4c4b-b8a0-9c6358c854d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
